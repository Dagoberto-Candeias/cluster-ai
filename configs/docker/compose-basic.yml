services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.30
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_API_URL=http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

  nginx:
    image: nginx:stable
    container_name: openwebui-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - open-webui
    restart: unless-stopped

# Nota: este docker-compose assume que o binário 'ollama' está rodando no host
# e escutando em http://localhost:11434. Em Linux, confirme suporte a host.docker.internal
# (Docker engine >= 20.10) ou ajuste extra_hosts conforme necessário.
