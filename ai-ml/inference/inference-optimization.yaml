apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-ai-onnx-runtime
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: onnx-runtime
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: onnx-runtime
  template:
    metadata:
      labels:
        app.kubernetes.io/name: onnx-runtime
        app.kubernetes.io/component: ml-inference
    spec:
      containers:
      - name: onnx-runtime
        image: mcr.microsoft.com/onnxruntime/server:latest
        ports:
        - containerPort: 8001
          name: http
        env:
        - name: ONNX_MODEL_PATH
          value: "/models/cluster_ai_model.onnx"
        - name: ONNX_SERVER_PORT
          value: "8001"
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
        - name: model-storage
          mountPath: /models
        livenessProbe:
          httpGet:
            path: "/v1/models"
            port: 8001
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: "/v1/models"
            port: 8001
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: cluster-ai-models-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: cluster-ai-onnx-runtime
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: onnx-runtime
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  ports:
  - name: http
    port: 8001
    targetPort: 8001
  selector:
    app.kubernetes.io/name: onnx-runtime
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-ai-tensorrt-server
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: tensorrt-server
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tensorrt-server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tensorrt-server
        app.kubernetes.io/component: ml-inference
    spec:
      containers:
      - name: tensorrt-server
        image: nvcr.io/nvidia/tensorrtserver:22.12-py3
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        - containerPort: 8002
          name: metrics
        env:
        - name: MODEL_REPOSITORY
          value: "/models"
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
            nvidia.com/gpu: 1
          limits:
            cpu: 2000m
            memory: 4Gi
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-storage
          mountPath: /models
        livenessProbe:
          httpGet:
            path: "/v2/health/ready"
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: "/v2/health/ready"
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: cluster-ai-models-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: cluster-ai-tensorrt-server
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: tensorrt-server
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 8000
  - name: grpc
    port: 8001
    targetPort: 8001
  - name: metrics
    port: 8002
    targetPort: 8002
  selector:
    app.kubernetes.io/name: tensorrt-server
  type: ClusterIP
---
apiVersion: batch/v1
kind: Job
metadata:
  name: cluster-ai-model-quantization
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: model-quantization
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: quantization
        image: python:3.11-slim
        command:
        - python
        - /scripts/quantize_model.py
        volumeMounts:
        - name: model-storage
          mountPath: /models
        - name: quantization-script
          mountPath: /scripts
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: cluster-ai-models-pvc
      - name: quantization-script
        configMap:
          name: cluster-ai-quantization-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-ai-quantization-config
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: model-quantization
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
data:
  quantize_model.py: |
    #!/usr/bin/env python3
    import torch
    import torch.quantization
    import onnxruntime as ort
    import numpy as np
    import os
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    class ModelQuantizer:
        def __init__(self):
            self.model_dir = "/models"

        def quantize_pytorch_model(self, model_path, quantized_path):
            """Quantize PyTorch model for better performance"""
            try:
                # Load model
                model = torch.load(model_path)
                model.eval()

                # Apply dynamic quantization
                quantized_model = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )

                # Save quantized model
                torch.save(quantized_model, quantized_path)
                logger.info(f"Quantized PyTorch model saved to {quantized_path}")

            except Exception as e:
                logger.error(f"Error quantizing PyTorch model: {e}")

        def convert_to_onnx(self, model_path, onnx_path):
            """Convert model to ONNX format"""
            try:
                # Load PyTorch model
                model = torch.load(model_path)
                model.eval()

                # Create dummy input
                dummy_input = torch.randn(1, 3, 224, 224)  # Adjust based on model input

                # Export to ONNX
                torch.onnx.export(
                    model,
                    dummy_input,
                    onnx_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
                )

                logger.info(f"Model converted to ONNX: {onnx_path}")

            except Exception as e:
                logger.error(f"Error converting to ONNX: {e}")

        def optimize_onnx_model(self, onnx_path, optimized_path):
            """Optimize ONNX model"""
            try:
                # Create session options for optimization
                sess_options = ort.SessionOptions()
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

                # Load and optimize model
                session = ort.InferenceSession(onnx_path, sess_options)

                # Save optimized model
                session.save_model_to_file(optimized_path)
                logger.info(f"ONNX model optimized: {optimized_path}")

            except Exception as e:
                logger.error(f"Error optimizing ONNX model: {e}")

        def process_models(self):
            """Process all models in the directory"""
            for filename in os.listdir(self.model_dir):
                if filename.endswith('.pt') or filename.endswith('.pth'):
                    model_path = os.path.join(self.model_dir, filename)
                    base_name = filename.rsplit('.', 1)[0]

                    # Quantize PyTorch model
                    quantized_path = os.path.join(self.model_dir, f"{base_name}_quantized.pt")
                    self.quantize_pytorch_model(model_path, quantized_path)

                    # Convert to ONNX
                    onnx_path = os.path.join(self.model_dir, f"{base_name}.onnx")
                    self.convert_to_onnx(model_path, onnx_path)

                    # Optimize ONNX
                    optimized_path = os.path.join(self.model_dir, f"{base_name}_optimized.onnx")
                    self.optimize_onnx_model(onnx_path, optimized_path)

    if __name__ == '__main__':
        quantizer = ModelQuantizer()
        quantizer.process_models()
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-ai-inference-gateway
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: inference-gateway
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: inference-gateway
  template:
    metadata:
      labels:
        app.kubernetes.io/name: inference-gateway
        app.kubernetes.io/component: ml-inference
    spec:
      containers:
      - name: inference-gateway
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
          name: http
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: nginx-config
        configMap:
          name: cluster-ai-inference-gateway-config
---
apiVersion: v1
kind: Service
metadata:
  name: cluster-ai-inference-gateway
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: inference-gateway
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app.kubernetes.io/name: inference-gateway
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-ai-inference-gateway-config
  namespace: cluster-ai
  labels:
    app.kubernetes.io/name: inference-gateway
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: cluster-ai
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }

    http {
        upstream tensorflow_serving {
            server cluster-ai-tensorflow-serving:8501;
        }

        upstream torchserve {
            server cluster-ai-torchserve:8080;
        }

        upstream onnx_runtime {
            server cluster-ai-onnx-runtime:8001;
        }

        upstream tensorrt_server {
            server cluster-ai-tensorrt-server:8000;
        }

        server {
            listen 80;

            # Route to TensorFlow Serving
            location /tensorflow/ {
                proxy_pass http://tensorflow_serving/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
            }

            # Route to TorchServe
            location /torchserve/ {
                proxy_pass http://torchserve/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
            }

            # Route to ONNX Runtime
            location /onnx/ {
                proxy_pass http://onnx_runtime/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
            }

            # Route to TensorRT Server
            location /tensorrt/ {
                proxy_pass http://tensorrt_server/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
            }

            # Health check
            location /health {
                access_log off;
                return 200 "inference gateway healthy\n";
                add_header Content-Type text/plain;
            }
        }
    }
