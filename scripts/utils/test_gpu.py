#!/usr/bin/env python3
"""
Script de teste para valida√ß√£o de GPU no Cluster AI
Testa PyTorch, TensorFlow e funcionalidades b√°sicas de GPU
"""

import os
import subprocess
import sys
from datetime import datetime

import torch


def run_command(cmd):
    """Executa comando e retorna resultado"""
    try:
        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=30
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Timeout"
    except Exception as e:
        return False, "", str(e)


def test_system_gpu():
    """Testa detec√ß√£o de GPU no sistema"""
    print("=== TESTE DE DETEC√á√ÉO DE GPU NO SISTEMA ===")

    # Testar lspci
    success, stdout, stderr = run_command(
        "lspci -nn | grep -i 'nvidia\\|amd' | head -10"
    )
    if success and stdout.strip():
        print("‚úÖ GPUs detectadas:")
        for line in stdout.strip().split("\n"):
            print(f"   {line}")
    else:
        print("‚ùå Nenhuma GPU dedicada detectada")

    # Testar nvidia-smi se dispon√≠vel
    success, stdout, stderr = run_command("which nvidia-smi")
    if success:
        success, stdout, stderr = run_command("nvidia-smi")
        if success:
            print("‚úÖ NVIDIA-SMI funcionando:")
            print(stdout[:200] + "..." if len(stdout) > 200 else stdout)
        else:
            print("‚ùå NVIDIA-SMI falhou")

    # Testar ROCm se dispon√≠vel
    if os.path.exists("/opt/rocm/bin/rocminfo"):
        success, stdout, stderr = run_command("/opt/rocm/bin/rocminfo --version")
        if success:
            print("‚úÖ ROCm detectado:", stdout.strip())
        else:
            print("‚ùå ROCm presente mas rocminfo falhou")


def test_pytorch_gpu():
    """Testa PyTorch com GPU"""
    print("\n=== TESTE PyTorch GPU ===")

    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")  # type: ignore
        print(f"Number of GPUs: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(
                f"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB"
            )

        # Teste b√°sico de tensor na GPU
        try:
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.matmul(x, y)
            print("‚úÖ Opera√ß√µes matriciais na GPU funcionando")
            print(f"  Result shape: {z.shape}, Device: {z.device}")
        except Exception as e:
            print(f"‚ùå Erro em opera√ß√µes GPU: {e}")
    else:
        print("‚ÑπÔ∏è  PyTorch usando CPU apenas")

        # Teste b√°sico na CPU
        try:
            x = torch.randn(1000, 1000)
            y = torch.randn(1000, 1000)
            z = torch.matmul(x, y)
            print("‚úÖ Opera√ß√µes matriciais na CPU funcionando")
        except Exception as e:
            print(f"‚ùå Erro em opera√ß√µes CPU: {e}")


def test_performance():
    """Teste de performance b√°sico"""
    print("\n=== TESTE DE PERFORMANCE ===")

    if torch.cuda.is_available():
        # Teste de velocidade GPU vs CPU
        size = 2000
        repetitions = 10

        # CPU
        start = datetime.now()
        for _ in range(repetitions):
            a = torch.randn(size, size)
            b = torch.randn(size, size)
            c = torch.matmul(a, b)
        cpu_time = (datetime.now() - start).total_seconds()

        # GPU
        start = datetime.now()
        for _ in range(repetitions):
            a = torch.randn(size, size).cuda()
            b = torch.randn(size, size).cuda()
            c = torch.matmul(a, b)
            torch.cuda.synchronize()  # Esperar GPU terminar
        gpu_time = (datetime.now() - start).total_seconds()

        print(f"CPU time: {cpu_time:.2f}s")
        print(f"GPU time: {gpu_time:.2f}s")
        if gpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"Speedup: {speedup:.1f}x")
        else:
            print("Speedup: N/A")
    else:
        print("‚ÑπÔ∏è  Teste de performance apenas CPU")
        size = 2000
        repetitions = 5

        start = datetime.now()
        for _ in range(repetitions):
            a = torch.randn(size, size)
            b = torch.randn(size, size)
            c = torch.matmul(a, b)
        cpu_time = (datetime.now() - start).total_seconds()

        print(f"CPU time: {cpu_time:.2f}s")


def test_memory():
    """Teste de mem√≥ria GPU"""
    print("\n=== TESTE DE MEM√ìRIA ===")

    if torch.cuda.is_available():
        try:
            # Alocar mem√≥ria
            torch.cuda.empty_cache()
            free_memory = torch.cuda.memory_allocated()
            print(f"Mem√≥ria GPU alocada: {free_memory / 1024**2:.1f} MB")

            # Alocar um tensor grande
            large_tensor = torch.randn(5000, 5000).cuda()
            allocated = torch.cuda.memory_allocated()
            print(f"Ap√≥s alocar tensor: {allocated / 1024**2:.1f} MB")

            # Liberar
            del large_tensor
            torch.cuda.empty_cache()
            print("‚úÖ Gerenciamento de mem√≥ria funcionando")

        except Exception as e:
            print(f"‚ùå Erro no teste de mem√≥ria: {e}")
    else:
        print("‚ÑπÔ∏è  Teste de mem√≥ria GPU n√£o dispon√≠vel")


def main():
    """Fun√ß√£o principal"""
    print("üß™ CLUSTER AI - TESTE DE GPU")
    print("=" * 50)

    try:
        test_system_gpu()
        test_pytorch_gpu()
        test_memory()
        test_performance()

        print("\n" + "=" * 50)
        print("‚úÖ Teste de GPU conclu√≠do!")

    except Exception as e:
        print(f"‚ùå Erro durante os testes: {e}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
