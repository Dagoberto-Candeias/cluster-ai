# Guia de Modelos de IA

## Vis√£o Geral
O Cluster AI suporta diversos tipos de modelos de IA atrav√©s do Ollama, organizados por categoria e caso de uso.

## Categorias de Modelos

### ü§ñ Modelos de Linguagem (LLM)
Modelos para processamento de texto, chat e gera√ß√£o de conte√∫do.

#### Chat Geral
```bash
# Llama 3 - Meta (Recomendado para portugu√™s)
ollama pull llama3:8b      # 8B par√¢metros, bom equil√≠brio
ollama pull llama3:70b     # 70B par√¢metros, alta qualidade

# Mistral - Mistral AI
ollama pull mistral:7b     # 7B par√¢metros, r√°pido
ollama pull mixtral:8x7b   # Mixtral 8x7B, excelente qualidade
```

#### C√≥digo e Desenvolvimento
```bash
# CodeLlama - Meta
ollama pull codellama:7b   # Gera√ß√£o de c√≥digo
ollama pull codellama:13b  # Melhor qualidade

# DeepSeek Coder
ollama pull deepseek-coder:6.7b  # Especializado em c√≥digo
```

#### An√°lise e Racioc√≠nio
```bash
# Qwen - Alibaba
ollama pull qwen:7b        # Forte em matem√°tica/l√≥gica
ollama pull qwen:72b       # Vers√£o grande

# Phi-2 - Microsoft
ollama pull phi:2.7b       # Modelo compacto, bom racioc√≠nio
```

### üëÅÔ∏è Modelos Multimodais (Vis√£o + Texto)
Modelos que processam imagens e texto simultaneamente.

```bash
# LLaVA - multimodal
ollama pull llava:7b       # Vis√£o b√°sica
ollama pull llava:13b      # Melhor qualidade

# BakLLaVA
ollama pull bakllava:7b    # Alternativa multimodal
```

### üé® Modelos de Gera√ß√£o de Imagens
Para gera√ß√£o de imagens (via integra√ß√£o externa).

```bash
# Stable Diffusion (via WebUI)
# Configurado automaticamente no cluster
# Acesse: http://localhost:3000
```

## Instala√ß√£o Automatizada
```bash
# Instalar modelos por categoria
./scripts/download_models.sh --category llm
./scripts/download_models.sh --category vision
./scripts/download_models.sh --category code

# Instalar modelo espec√≠fico
./scripts/download_models.sh --model llama3:8b

# Listar modelos instalados
ollama list
```

## Uso dos Modelos

### Via OpenWebUI (Interface Web)
```bash
# Acesse: http://localhost:3000
# Selecione modelo no dropdown
# Digite sua pergunta/conversa
```

### Via API REST
```python
from ollama import Client

client = Client(host='http://localhost:11434')

# Chat simples
response = client.chat(
    model='llama3:8b',
    messages=[{'role': 'user', 'content': 'Ol√°!'}]
)

# Streaming
for chunk in client.chat(
    model='llama3:8b',
    messages=[{'role': 'user', 'content': 'Conte uma hist√≥ria'}],
    stream=True
):
    print(chunk['message']['content'], end='')
```

### Via Dask (Processamento Distribu√≠do)
```python
from dask.distributed import Client
import ollama

# Conectar ao cluster
client = Client('localhost:8786')

def process_with_ai(text, model='llama3:8b'):
    response = ollama.chat(
        model=model,
        messages=[{'role': 'user', 'content': text}]
    )
    return response['message']['content']

# Processar m√∫ltiplos textos em paralelo
texts = ["Texto 1", "Texto 2", "Texto 3"]
futures = client.map(process_with_ai, texts)
results = client.gather(futures)
```

## Gerenciamento de Modelos

### Verificar Status
```bash
# Listar modelos
ollama list

# Ver informa√ß√µes detalhadas
ollama show llama3:8b

# Ver uso de disco
du -sh ~/.ollama/models/
```

### Limpeza e Otimiza√ß√£o
```bash
# Remover modelo espec√≠fico
ollama rm llama3:8b

# Limpar modelos n√£o usados (30+ dias)
./scripts/ollama/model_manager.sh cleanup 30

# Otimizar cache
./scripts/ollama/model_manager.sh optimize
```

### Cache e Performance
```bash
# Pr√©-carregar modelos frequentemente usados
ollama pull llama3:8b
ollama pull mistral:7b

# Verificar cache hits
./scripts/monitoring/advanced_dashboard.sh live
```

## Recomenda√ß√µes por Caso de Uso

### üí¨ Chat e Assistente Virtual
- **Prim√°rio**: llama3:8b ou mistral:7b
- **Backup**: qwen:7b
- **Para portugu√™s**: Prefira llama3 (treinado em m√∫ltiplos idiomas)

### üë®‚Äçüíª Desenvolvimento de Software
- **Prim√°rio**: codellama:7b ou deepseek-coder:6.7b
- **Backup**: llama3:8b
- **Para debugging**: Use modelos com contexto longo

### üìä An√°lise de Dados
- **Prim√°rio**: qwen:7b (forte em matem√°tica)
- **Backup**: llama3:8b
- **Para visualiza√ß√£o**: Combine com modelos de vis√£o

### üé® Cria√ß√£o de Conte√∫do
- **Prim√°rio**: llama3:70b ou mixtral:8x7b
- **Backup**: qwen:72b
- **Para imagens**: llava + Stable Diffusion

## Solu√ß√£o de Problemas

### Modelo N√£o Carrega
```bash
# Verificar espa√ßo em disco
df -h

# Verificar RAM dispon√≠vel
free -h

# Reiniciar Ollama
systemctl restart ollama

# Limpar cache
ollama prune
```

### Performance Lenta
```bash
# Usar modelo menor
ollama pull llama3:8b  # ao inv√©s de 70b

# Verificar recursos do sistema
./scripts/monitoring/resource_monitor.py

# Otimizar worker
dask-worker --memory-limit 4GB --nthreads 4
```

### Erro de Mem√≥ria
```bash
# Reduzir contexto
ollama run llama3:8b --format json '{"options": {"num_ctx": 2048}}'

# Usar quantiza√ß√£o
ollama pull llama3:8b-q4_0  # Vers√£o quantizada
```

## Pr√≥ximos Passos
- Explore integra√ß√£o com Dask para processamento paralelo
- Teste combina√ß√µes de modelos para tarefas complexas
- Monitore uso de recursos e otimize conforme necess√°rio
- Considere fine-tuning para casos espec√≠ficos
