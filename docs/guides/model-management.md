# üìö Gerenciamento de Modelos - Cluster AI

Este guia detalha como gerenciar modelos de IA no Cluster AI, incluindo instala√ß√£o, categoriza√ß√£o, otimiza√ß√£o e monitoramento.

## üè∑Ô∏è Categorias de Modelos

O Cluster AI suporta diferentes categorias de modelos para atender a diversos casos de uso:

### ü§ñ Modelos LLM (Large Language Models)
- **Uso Principal**: Processamento de linguagem natural, chatbots, gera√ß√£o de texto.
- **Exemplos**:
  - `llama3:8b`: Conversa√ß√£o geral e tarefas cognitivas.
  - `mistral:7b`: An√°lise t√©cnica e racioc√≠nio l√≥gico.
  - `codellama:7b`: Gera√ß√£o e depura√ß√£o de c√≥digo.
  - `deepseek-coder:6.7b`: Desenvolvimento de software avan√ßado.
  - `phi3:3.8b`: Modelo leve para dispositivos com recursos limitados.
- **Requisitos**: 4-16GB RAM, suporte a AVX2 recomendado.

### üëÅÔ∏è Modelos de Vis√£o
- **Uso Principal**: An√°lise de imagens, reconhecimento visual, OCR.
- **Exemplos**:
  - `llava:7b`: Descri√ß√£o e an√°lise de imagens.
  - `bakllava:7b`: Processamento multimodal avan√ßado.
  - `moondream:1.8b`: An√°lise leve de imagens para mobile.
- **Requisitos**: GPU recomendada para infer√™ncia r√°pida.

### üîÑ Modelos Multimodal
- **Uso Principal**: Combina√ß√£o de texto e imagem, an√°lise integrada.
- **Exemplos**:
  - `llava-llama3:8b`: Vis√£o + linguagem baseada em Llama3.
  - `llava-mistral:7b`: Capacidades t√©cnicas multimodais.
- **Requisitos**: 8-32GB RAM, suporte a CUDA/ROCm para performance.

### üìä Modelos Especializados
- **Uso Principal**: Tarefas espec√≠ficas como embeddings, classifica√ß√£o.
- **Exemplos**:
  - `nomic-embed-text:1.5b`: Gera√ß√£o de embeddings para busca sem√¢ntica.
  - `all-minilm:384`: Classifica√ß√£o de texto leve.

## üöÄ Instala√ß√£o de Modelos

### Instala√ß√£o Autom√°tica
```bash
# Instalar por categoria
./scripts/download_models.sh --category llm
./scripts/download_models.sh --category vision
./scripts/download_models.sh --category multimodal

# Instalar pacotes recomendados
./scripts/download_models.sh --category recommended

# Instalar todos os modelos
./scripts/download_models.sh --category all --parallel

# Verificar instala√ß√£o
ollama list
```

### Instala√ß√£o Manual
```bash
# Exemplo: Instalar Llama3
ollama pull llama3:8b

# Testar modelo
ollama run llama3:8b "Explique machine learning em uma frase."

# Instalar m√∫ltiplos em paralelo (requer configura√ß√£o)
ollama pull llama3:8b & ollama pull mistral:7b &
```

### Configura√ß√£o Avan√ßada
- **Par√¢metros de Pull**:
  ```bash
  # Especificar tag e plataforma
  ollama pull llama3:8b --platform linux/amd64
  
  # Configurar proxy (se necess√°rio)
  export OLLAMA_PROXY=http://proxy.example.com:8080
  ollama pull mistral:7b
  ```
- **Espa√ßo em Disco**: Modelos ocupam 4-50GB cada. Monitore com `df -h`.

## ‚öôÔ∏è Configura√ß√£o e Otimiza√ß√£o

### Par√¢metros de Execu√ß√£o
Edite `config/ollama.conf` para configura√ß√µes globais:
```ini
# config/ollama.conf
[ollama]
num_parallel = 4
num_gpu = 1
gpu_memory_limit = 4GB
keep_alive = 5m
```

### Otimiza√ß√£o por Hardware
- **CPU Only**:
  ```bash
  ollama run llama3:8b --num-thread 8
  ```
- **GPU NVIDIA**:
  ```bash
  export CUDA_VISIBLE_DEVICES=0
  ollama run llava:7b
  ```
- **GPU AMD**:
  ```bash
  export HSA_OVERRIDE_GFX_VERSION=10.3.0  # Para ROCm
  ollama run mistral:7b
  ```

### Cache e Gerenciamento de Mem√≥ria
- **Limpeza Autom√°tica**:
  ```bash
  # Limpar modelos n√£o usados h√° 30 dias
  ./scripts/ollama/model_manager.sh cleanup 30
  
  # Otimizar uso de disco
  ./scripts/ollama/model_manager.sh optimize
  ```
- **Cache Distribu√≠do**: Modelos s√£o cacheados em workers para infer√™ncia paralela.

## üìä Monitoramento de Modelos

### M√©tricas Dispon√≠veis
- **Uso de Mem√≥ria**: Monitoramento em tempo real via dashboard.
- **Lat√™ncia de Infer√™ncia**: M√©tricas de resposta por modelo.
- **Taxa de Erro**: Detec√ß√£o de falhas em prompts.

### Comandos de Monitoramento
```bash
# Listar modelos com m√©tricas
./scripts/ollama/model_manager.sh list --metrics

# Monitorar uso em tempo real
./scripts/monitoring/model_monitor.sh live

# Relat√≥rio de performance
./scripts/ollama/model_manager.sh report --format json
```

### Integra√ß√£o com Dashboard
- Acesse `http://localhost:3000/models` para visualiza√ß√£o gr√°fica.
- Alertas autom√°ticos para sobrecarga de modelos.

## üîß Solu√ß√£o de Problemas

### Erros Comuns
- **"Out of memory"**:
  - Solu√ß√£o: Reduza `num_parallel` ou use modelo menor.
  - Comando: `ollama run phi3:3.8b` (leve).

- **"Model not found"**:
  - Solu√ß√£o: Verifique `ollama list` e reinstale se necess√°rio.
  - Comando: `ollama pull <modelo> --force`.

- **Performance lenta**:
  - Solu√ß√£o: Ative GPU ou otimize threads.
  - Verifique: `./scripts/optimization/model_optimizer.sh`.

### Logs e Debugging
```bash
# Ver logs do Ollama
tail -f ~/.ollama/logs/server.log

# Logs detalhados
OLLAMA_DEBUG=1 ollama run llama3:8b "Teste"
```

## üõ°Ô∏è Melhores Pr√°ticas

1. **Comece Pequeno**: Instale modelos leves primeiro (phi3, moondream).
2. **Monitore Recursos**: Use dashboard para evitar sobrecarga.
3. **Atualize Regularmente**: `ollama pull <modelo> --update`.
4. **Backup de Modelos**: `ollama export <modelo> model.tar`.
5. **Seguran√ßa**: Use modelos de fontes confi√°veis (Ollama oficial).

## üìà Roadmap de Modelos

- **v1.1**: Suporte a fine-tuning local.
- **v1.2**: Integra√ß√£o com Hugging Face Hub.
- **v2.0**: Modelos customizados via API.

Para mais detalhes, consulte [Ollama Documentation](https://ollama.ai/docs).

*√öltima atualiza√ß√£o: 2025-01-28*
