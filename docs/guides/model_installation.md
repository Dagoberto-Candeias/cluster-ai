# Guia de Instala√ß√£o de Modelos - Cluster AI

## Vis√£o Geral

Este guia explica como instalar e gerenciar modelos de IA no Cluster AI, com foco em otimiza√ß√£o de recursos e sele√ß√£o inteligente de modelos.

## Categorias de Modelos

### ü§ñ Modelos de Conversa√ß√£o Geral
Modelos vers√°teis para chat, escrita e tarefas gerais.

#### Recomendados para Iniciantes:
- **llama3.1:8b** - Meta Llama 3.1 (8B par√¢metros)
  - Uso: Chat geral, escrita criativa
  - Recursos: ~4.7GB, CPU/GPU
  - Pontos Fortes: Equilibrado, multil√≠ngue

- **llama3:8b** - Meta Llama 3 (8B par√¢metros)
  - Uso: Chat geral, an√°lise de texto
  - Recursos: ~4.7GB, CPU/GPU
  - Pontos Fortes: Est√°vel, bem testado

- **mistral:7b** - Mistral AI 7B
  - Uso: Chat t√©cnico, programa√ß√£o
  - Recursos: ~4.1GB, CPU/GPU
  - Pontos Fortes: C√≥digo, matem√°tica

### üé® Modelos Criativos
Especializados em gera√ß√£o de conte√∫do criativo.

#### Modelos Dispon√≠veis:
- **llava:7b** - LLaVA (Large Language and Vision Assistant)
  - Uso: An√°lise de imagens + texto
  - Recursos: ~4.6GB, GPU recomendado
  - Pontos Fortes: Multimodal, descri√ß√µes

- **deepseek-coder:6.7b** - DeepSeek Coder
  - Uso: Gera√ß√£o de c√≥digo, debugging
  - Recursos: ~3.8GB, CPU/GPU
  - Pontos Fortes: Sintaxe perfeita, m√∫ltiplas linguagens

### üß† Modelos Avan√ßados
Modelos de alta performance para tarefas complexas.

#### Modelos Pesados:
- **mixtral:8x7b** - Mixtral 8x7B
  - Uso: An√°lise complexa, pesquisa
  - Recursos: ~26GB, GPU obrigat√≥ria
  - Pontos Fortes: Racioc√≠nio avan√ßado, multitarefa

- **codellama:13b** - Code Llama 13B
  - Uso: Desenvolvimento avan√ßado
  - Recursos: ~7.4GB, GPU recomendado
  - Pontos Fortes: Arquiteturas complexas, documenta√ß√£o

### üåç Modelos Especializados
Modelos otimizados para tarefas espec√≠ficas.

#### Modelos por Dom√≠nio:
- **phi3:3.8b** - Microsoft Phi-3
  - Uso: Matem√°tica, ci√™ncia
  - Recursos: ~2.2GB, CPU eficiente
  - Pontos Fortes: R√°pido, compacto

- **gemma2:9b** - Google Gemma 2
  - Uso: Pesquisa acad√™mica, an√°lise
  - Recursos: ~5.2GB, GPU recomendado
  - Pontos Fortes: Precis√£o, factualidade

## Instala√ß√£o Autom√°tica

### M√©todo 1: Instala√ß√£o Inteligente (Recomendado)
```bash
# Executar instala√ß√£o principal
./install_unified.sh

# Durante a instala√ß√£o, selecionar modelos:
# - Op√ß√£o: "Selecionar modelos espec√≠ficos"
# - Escolher baseado no uso pretendido
```

### M√©todo 2: Instala√ß√£o Manual
```bash
# Verificar modelos dispon√≠veis
ollama list

# Instalar modelo espec√≠fico
ollama pull llama3.1:8b

# Verificar instala√ß√£o
ollama list
```

### M√©todo 3: Instala√ß√£o em Lote
```bash
# Instalar conjunto recomendado
./scripts/utils/check_models.sh

# Seguir menu interativo para sele√ß√£o
```

## Sele√ß√£o de Modelos por Caso de Uso

### üíª Desenvolvimento de Software
```
Prioridade:
1. deepseek-coder:6.7b (c√≥digo)
2. codellama:13b (arquiteturas)
3. llama3.1:8b (documenta√ß√£o)
```

### üìä An√°lise de Dados
```
Prioridade:
1. llama3.1:8b (an√°lise geral)
2. mixtral:8x7b (complexa)
3. gemma2:9b (precis√£o)
```

### üé® Cria√ß√£o de Conte√∫do
```
Prioridade:
1. llama3:8b (escrita criativa)
2. llava:7b (multimodal)
3. mistral:7b (t√©cnico)
```

### üî¨ Pesquisa Acad√™mica
```
Prioridade:
1. mixtral:8x7b (racioc√≠nio)
2. gemma2:9b (factual)
3. phi3:3.8b (matem√°tica)
```

## Otimiza√ß√£o de Recursos

### Para Hardware Limitado (CPU/RAM)
```bash
# Modelos leves recomendados
ollama pull phi3:3.8b        # 2.2GB
ollama pull gemma2:2b        # 1.6GB
ollama pull llama3:8b        # 4.7GB
```

### Para GPU Dispon√≠vel
```bash
# Modelos otimizados para GPU
ollama pull llama3.1:8b      # Suporte CUDA
ollama pull mixtral:8x7b     # Alto paralelismo
ollama pull codellama:13b    # Acelera√ß√£o GPU
```

### Configura√ß√£o de Mem√≥ria
```bash
# Ajustar limite de mem√≥ria por modelo
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_MAX_QUEUE=512
```

## Gerenciamento de Modelos

### Listar Modelos Instalados
```bash
ollama list
```

### Verificar Uso de Espa√ßo
```bash
# Espa√ßo usado pelos modelos
du -sh ~/.ollama/models/

# Detalhes por modelo
ollama list --size
```

### Remover Modelos N√£o Usados
```bash
# Listar modelos
ollama list

# Remover modelo espec√≠fico
ollama rm modelo-nome:tag

# Limpar cache
ollama cache clean
```

### Atualizar Modelos
```bash
# Verificar atualiza√ß√µes dispon√≠veis
ollama list --updates

# Atualizar modelo espec√≠fico
ollama pull modelo-nome:tag --update
```

## Configura√ß√£o Avan√ßada

### Personaliza√ß√£o de Par√¢metros
```bash
# Arquivo de configura√ß√£o
~/.ollama/config.yaml

# Exemplo de configura√ß√£o
models:
  - name: llama3.1:8b
    parameters:
      num_ctx: 4096
      num_thread: 8
      num_gpu: 1
```

### Otimiza√ß√£o de Performance
```bash
# Vari√°veis de ambiente
export OLLAMA_NUM_THREAD=8
export OLLAMA_NUM_GPU=1
export OLLAMA_MAX_LOADED_MODELS=3
export OLLAMA_HOST=0.0.0.0:11434
```

### Configura√ß√£o para Cluster
```bash
# Configura√ß√£o distribu√≠da
export OLLAMA_LOAD_TIMEOUT=300
export OLLAMA_RUNNERS_DIR=/opt/ollama/runners
export OLLAMA_TMPDIR=/tmp/ollama
```

## Solu√ß√£o de Problemas

### Modelo N√£o Carrega
```bash
# Verificar logs
tail -f ~/.ollama/logs/server.log

# Verificar recursos
free -h
nvidia-smi  # se GPU

# Reiniciar servi√ßo
systemctl restart ollama
```

### Performance Lenta
```bash
# Otimizar threads
export OLLAMA_NUM_THREAD=4

# Usar GPU se dispon√≠vel
export OLLAMA_NUM_GPU=1

# Reduzir contexto
export OLLAMA_NUM_CTX=2048
```

### Erro de Mem√≥ria
```bash
# Limitar modelos carregados
export OLLAMA_MAX_LOADED_MODELS=1

# Usar modelos menores
ollama pull phi3:3.8b
ollama pull gemma2:2b
```

### Problemas de Rede
```bash
# Configurar proxy se necess√°rio
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# Verificar conectividade
curl -I https://registry.ollama.ai
```

## Monitoramento e Manuten√ß√£o

### Scripts de Monitoramento
```bash
# Verificar status dos modelos
./scripts/ollama/model_manager.sh status

# Limpeza autom√°tica
./scripts/ollama/model_manager.sh cleanup

# Otimiza√ß√£o de cache
./scripts/ollama/model_manager.sh optimize
```

### Logs e Diagn√≥stico
```bash
# Logs do Ollama
tail -f ~/.ollama/logs/server.log

# Logs do Cluster AI
tail -f logs/ollama_download.log

# Diagn√≥stico completo
./scripts/health_check_enhanced_fixed.sh
```

## Recomenda√ß√µes por Perfil

### üë®‚Äçüíª Desenvolvedor Solo
```
Modelos Essenciais:
- deepseek-coder:6.7b (c√≥digo)
- llama3:8b (documenta√ß√£o)

Recursos: 8GB RAM, CPU moderno
```

### üè¢ Equipe de Desenvolvimento
```
Modelos Essenciais:
- codellama:13b (arquiteturas)
- mixtral:8x7b (an√°lise)
- deepseek-coder:6.7b (c√≥digo)

Recursos: 16GB+ RAM, GPU recomendada
```

### üî¨ Pesquisador/Acad√™mico
```
Modelos Essenciais:
- mixtral:8x7b (racioc√≠nio complexo)
- gemma2:9b (precis√£o factual)
- phi3:3.8b (matem√°tica)

Recursos: 32GB+ RAM, GPU obrigat√≥ria
```

### üéØ Usu√°rio Geral
```
Modelos Essenciais:
- llama3.1:8b (conversa√ß√£o)
- llava:7b (multimodal)
- mistral:7b (tarefas gerais)

Recursos: 8GB RAM m√≠nimo
```

## Atualiza√ß√µes e Manuten√ß√£o

### Verificar Atualiza√ß√µes
```bash
# Atualiza√ß√µes do Ollama
ollama --version

# Modelos desatualizados
ollama list --outdated
```

### Backup de Modelos
```bash
# Backup do diret√≥rio de modelos
tar -czf ollama_models_backup.tar.gz ~/.ollama/models/

# Restaura√ß√£o
tar -xzf ollama_models_backup.tar.gz -C ~/
```

### Limpeza Regular
```bash
# Remover modelos n√£o usados h√° 30+ dias
./scripts/ollama/model_manager.sh cleanup --older-than 30

# Otimizar armazenamento
./scripts/ollama/model_manager.sh optimize
```

---

**üìã Checklist de Instala√ß√£o:**
- [ ] Avaliar recursos dispon√≠veis (CPU, RAM, GPU)
- [ ] Definir caso de uso principal
- [ ] Selecionar modelos apropriados
- [ ] Verificar espa√ßo em disco necess√°rio
- [ ] Executar instala√ß√£o
- [ ] Testar funcionalidade
- [ ] Configurar otimiza√ß√£o de performance

**üîó Links √öteis:**
- [Documenta√ß√£o Ollama](https://github.com/ollama/ollama)
- [Modelos Dispon√≠veis](https://ollama.ai/library)
- [Guia de Performance](https://github.com/ollama/ollama/blob/main/docs/faq.md)

**üìû Suporte:**
Para problemas espec√≠ficos, consulte os logs em `logs/ollama_download.log` e `~/.ollama/logs/`
