# üß† Guia Definitivo Ollama para Programa√ß√£o e IA

Este guia consolida **todos os modelos, configura√ß√µes e integra√ß√µes** discutidas, com instru√ß√µes completas para instala√ß√£o, gerenciamento e uso avan√ßado do Ollama em desenvolvimento de software.

---

## üìå √çndice

1. [Instala√ß√£o e Configura√ß√£o B√°sica](#-instala√ß√£o-e-configura√ß√£o-b√°sica)
2. [Modelos Dispon√≠veis por Categoria](#-modelos-dispon√≠veis-por-categoria)
3. [Comandos Essenciais](#-comandos-essenciais)
4. [Integra√ß√£o com Open WebUI](#-integra√ß√£o-com-open-webui)
5. [Integra√ß√£o com VSCode (Continue)](#-integra√ß√£o-com-vscode-continue)
6. [Prompts Especializados para Devs](#-prompts-especializados-para-devs)
7. [Otimiza√ß√£o de Performance](#-otimiza√ß√£o-de-performance)
8. [Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)
9. [Scripts de Automa√ß√£o](#-scripts-de-automa√ß√£o)
10. [Recursos Adicionais](#-recursos-adicionais)

---

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o B√°sica

### Pr√©-requisitos
```bash
# Verificar se tem curl
curl --version

# Verificar espa√ßo em disco (recomendado: 50GB+)
df -h
```

### Instala√ß√£o no Linux/macOS
```bash
# Download e instala√ß√£o autom√°tica
curl -fsSL https://ollama.com/install.sh | sh

# Verificar instala√ß√£o
ollama --version

# Iniciar servi√ßo
ollama serve

# Verificar se est√° rodando
curl http://localhost:11434/api/tags
```

### Instala√ß√£o no Windows
```powershell
# Download do instalador
irm https://ollama.com/download/OllamaSetup.exe -OutFile .\OllamaSetup.exe

# Executar instalador
.\OllamaSetup.exe

# Verificar instala√ß√£o
ollama --version
```

### Configura√ß√£o de Ambiente
```bash
# Adicionar ao ~/.bashrc ou ~/.zshrc
export OLLAMA_NUM_GPU_LAYERS=35
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_HOST=0.0.0.0:11434

# Para sistemas com pouca mem√≥ria
export OLLAMA_NUM_GPU_LAYERS=25
export OLLAMA_MAX_LOADED_MODELS=1
```

---

## üìö Modelos Dispon√≠veis por Categoria

### 1Ô∏è‚É£ Modelos de Chat (Conversa√ß√£o Geral)

| Modelo | Tamanho | Destaque | Comando Pull | Comando Run |
|--------|---------|----------|--------------|-------------|
| `llama3.1:8b` | ~5GB | Melhor equil√≠brio geral | `ollama pull llama3.1:8b` | `ollama run llama3.1:8b` |
| `llama3.2` | ~5GB | Contexto avan√ßado | `ollama pull llama3.2` | `ollama run llama3.2` |
| `llama3:8b` | ~5GB | Assistente virtual | `ollama pull llama3:8b` | `ollama run llama3:8b` |
| `phi3` | ~2GB | Racioc√≠nio l√≥gico | `ollama pull phi3` | `ollama run phi3` |
| `phi4` | ~9GB | Infer√™ncia complexa | `ollama pull phi4` | `ollama run phi4` |
| `qwen3` | ~5GB | Performance alta | `ollama pull qwen3` | `ollama run qwen3` |
| `mistral` | ~4GB | Chat interativo | `ollama pull mistral` | `ollama run mistral` |
| `gemma2:9b` | ~5GB | Conversa multi-turn | `ollama pull gemma2:9b` | `ollama run gemma2:9b` |
| `deepseek-chat` | ~9GB | T√©cnico especializado | `ollama pull deepseek-chat` | `ollama run deepseek-chat` |
| `llama2` | ~4GB | NLP e chat | `ollama pull llama2` | `ollama run llama2` |

### 2Ô∏è‚É£ Modelos de Programa√ß√£o (C√≥digo)

| Modelo | Tamanho | Destaque | Comando Pull | Comando Run |
|--------|---------|----------|--------------|-------------|
| `deepseek-coder-v2:16b` | ~9GB | **MELHOR PARA DEV** - 128K contexto | `ollama pull deepseek-coder-v2:16b` | `ollama run deepseek-coder-v2:16b` |
| `codellama-34b` | ~34GB | Alta precis√£o (92.3% HumanEval) | `ollama pull codellama-34b` | `ollama run codellama-34b` |
| `codellama-7b` | ~4GB | Vers√£o leve | `ollama pull codellama-7b` | `ollama run codellama-7b` |
| `starcoder2:7b` | ~4GB | Autocomplete IDE | `ollama pull starcoder2:7b` | `ollama run starcoder2:7b` |
| `starcoder2:3b` | ~2GB | Autocomplete leve | `ollama pull starcoder2:3b` | `ollama run starcoder2:3b` |
| `qwen2.5-coder:1.5b` | ~1GB | CPU-friendly | `ollama pull qwen2.5-coder:1.5b` | `ollama run qwen2.5-coder:1.5b` |
| `deepseek-coder` | ~800MB | Leve e r√°pido | `ollama pull deepseek-coder` | `ollama run deepseek-coder` |

### 3Ô∏è‚É£ Modelos Multimodais (Vis√£o + Texto)

| Modelo | Tamanho | Destaque | Comando Pull | Comando Run |
|--------|---------|----------|--------------|-------------|
| `llava` | ~5GB | An√°lise de imagens + chat | `ollama pull llava` | `ollama run llava` |
| `qwen2-vl` | ~6GB | Multimodal avan√ßado | `ollama pull qwen2-vl` | `ollama run qwen2-vl` |
| `granite3-moe` | ~7GB | Vis√£o + NLP | `ollama pull granite3-moe` | `ollama run granite3-moe` |
| `minicpm-v` | ~3GB | Compacto | `ollama pull minicpm-v` | `ollama run minicpm-v` |
| `deepseek-vision` | ~9GB | OCR t√©cnico | `ollama pull deepseek-vision` | `ollama run deepseek-vision` |
| `gemma2:27b` | ~14GB | Multimodal grande | `ollama pull gemma2:27b` | `ollama run gemma2:27b` |
| `codegemma` | ~5GB | C√≥digo + vis√£o | `ollama pull codegemma` | `ollama run codegemma` |

### 4Ô∏è‚É£ Modelos de Embeddings

| Modelo | Tamanho | Destaque | Comando Pull | Comando Run |
|--------|---------|----------|--------------|-------------|
| `nomic-embed-text` | ~300MB | Busca sem√¢ntica | `ollama pull nomic-embed-text` | `ollama run nomic-embed-text` |
| `text-embedding-3-large` | ~2GB | Embeddings grandes | `ollama pull text-embedding-3-large` | `ollama run text-embedding-3-large` |
| `text-embedding-3-small` | ~500MB | Embeddings leves | `ollama pull text-embedding-3-small` | `ollama run text-embedding-3-small` |
| `gemma-embed` | ~3GB | Embeddings Gemma | `ollama pull gemma-embed` | `ollama run gemma-embed` |
| `qwen-embed` | ~2GB | Embeddings Qwen | `ollama pull qwen-embed` | `ollama run qwen-embed` |
| `mistral-embed` | ~2GB | Embeddings Mistral | `ollama pull mistral-embed` | `ollama run mistral-embed` |
| `llama-embed-7b` | ~4GB | Embeddings LLaMA | `ollama pull llama-embed-7b` | `ollama run llama-embed-7b` |
| `phi-embed` | ~2GB | Embeddings Phi | `ollama pull phi-embed` | `ollama run phi-embed` |

### 5Ô∏è‚É£ Modelos Leves (Edge/CPU)

| Modelo | Tamanho | Destaque | Comando Pull | Comando Run |
|--------|---------|----------|--------------|-------------|
| `tinyllama:1b` | ~1GB | Raspberry Pi | `ollama pull tinyllama:1b` | `ollama run tinyllama:1b` |
| `phi3` | ~2GB | Laptops/CPU | `ollama pull phi3` | `ollama run phi3` |

---

## ‚ö° Comandos Essenciais

### Gerenciamento B√°sico
```bash
# Listar modelos instalados
ollama list

# Baixar modelo
ollama pull <modelo>

# Executar modelo interativo
ollama run <modelo>

# Executar com prompt direto
ollama run phi3 --prompt "Explique este c√≥digo Python"

# Remover modelo
ollama rm <modelo>

# Ver informa√ß√µes do modelo
ollama show <modelo>

# Listar modelos carregados
ollama ps

# Limpar cache
ollama clean
```

### Comandos Avan√ßados
```bash
# Executar em porta espec√≠fica
ollama serve --port 11435

# For√ßar uso de CPU
OLLAMA_NO_CUDA=1 ollama run <modelo>

# Limitar camadas GPU
OLLAMA_NUM_GPU_LAYERS=25 ollama run <modelo>

# Atualizar todos os modelos
ollama list | awk '{print $1}' | xargs -I {} ollama pull {}
```

---

## üñ•Ô∏è Integra√ß√£o com Open WebUI

### Instala√ß√£o do Open WebUI
```bash
# Via Docker (recomendado)
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

# Ou via script
curl -fsSL https://raw.githubusercontent.com/open-webui/open-webui/main/install.sh | sh
```

### Configura√ß√£o B√°sica
1. Acesse http://localhost:3000
2. V√° em **Settings** ‚Üí **Models**
3. Configure:
   - **Backend**: Ollama
   - **API URL**: http://localhost:11434
   - **Model**: deepseek-coder-v2:16b (ou outro)

### Configura√ß√£o Avan√ßada
```yaml
# Par√¢metros recomendados
temperature: 0.3          # Precis√£o para c√≥digo
max_tokens: 8192          # Contexto longo
top_p: 0.9               # Criatividade controlada
frequency_penalty: 0.1    # Evitar repeti√ß√µes
presence_penalty: 0.1     # Diversidade
```

### Modelos Recomendados por Uso
```bash
# Desenvolvimento
ollama pull deepseek-coder-v2:16b

# Chat geral
ollama pull llama3.1:8b

# An√°lise de imagens
ollama pull llava

# Processamento de texto
ollama pull nomic-embed-text
```

---

## üíª Integra√ß√£o com VSCode (Continue)

### Instala√ß√£o da Extens√£o
1. Abra VSCode
2. Extensions ‚Üí Busque "Continue"
3. Instale e reinicie

### Configura√ß√£o Completa (.continue/config.json)
```json
{
  "models": [
    {
      "name": "Assistente Principal",
      "provider": "ollama",
      "model": "deepseek-coder-v2:16b",
      "temperature": 0.3,
      "maxTokens": 8192,
      "contextLength": 131072,
      "roles": ["chat", "edit", "apply"],
      "systemMessage": "Voc√™ √© um engenheiro de software s√™nior. Responda em PT-BR com c√≥digo funcional."
    },
    {
      "name": "Revisor R√°pido",
      "provider": "ollama",
      "model": "phi3",
      "temperature": 0.2,
      "maxTokens": 4096,
      "roles": ["review"]
    },
    {
      "name": "Vision Assistant",
      "provider": "ollama",
      "model": "llava",
      "capabilities": ["image_input"],
      "roles": ["chat"],
      "temperature": 0.7,
      "maxTokens": 2048
    }
  ],
  "tabAutocomplete": {
    "model": "starcoder2:7b",
    "temperature": 0.2,
    "maxTokens": 512,
    "debounceDelay": 250
  },
  "context": [
    {
      "provider": "codebase",
      "params": {
        "nRetrieve": 25,
        "nFinal": 5
      }
    },
    {
      "provider": "file"
    },
    {
      "provider": "code"
    },
    {
      "provider": "diff"
    },
    {
      "provider": "folder"
    },
    {
      "provider": "open",
      "params": {
        "onlyPinned": false
      }
    },
    {
      "provider": "terminal"
    },
    {
      "provider": "problems"
    },
    {
      "provider": "tree"
    }
  ],
  "rules": [
    {
      "name": "Idioma Padr√£o",
      "rule": "Sempre responda em portugu√™s brasileiro, exceto quando especificamente solicitado outro idioma"
    },
    {
      "name": "Qualidade do C√≥digo",
      "rule": "Priorize c√≥digo limpo, leg√≠vel e bem documentado. Use padr√µes de nomenclatura consistentes e siga as melhores pr√°ticas da linguagem"
    },
    {
      "name": "Seguran√ßa e Performance",
      "rule": "Sempre considere aspectos de seguran√ßa e performance nas solu√ß√µes propostas. Identifique vulnerabilidades potenciais"
    },
    {
      "name": "Explica√ß√µes T√©cnicas",
      "rule": "Forne√ßa explica√ß√µes claras do racioc√≠nio por tr√°s das solu√ß√µes, especialmente para c√≥digo complexo"
    },
    {
      "name": "Python",
      "rule": "Para Python, use type hints, siga PEP 8, prefira f-strings e use dataclasses/pydantic para estruturas de dados",
      "globs": ["**/*.py"]
    },
    {
      "name": "TypeScript",
      "rule": "Para projetos TypeScript, sempre use tipagem forte, interfaces bem definidas e evite 'any'. Prefira type guards e utility types",
      "globs": ["**/*.{ts,tsx}"]
    },
    {
      "name": "React",
      "rule": "Para React, use hooks funcionais, evite prop drilling, implemente error boundaries e otimize re-renders",
      "globs": ["**/*.{jsx,tsx}"]
    },
    {
      "name": "Testes",
      "rule": "Para arquivos de teste, foque em cobertura completa, casos extremos, mocks apropriados e testes leg√≠veis",
      "globs": ["**/*.{test,spec}.{js,ts,jsx,tsx,py}"]
    }
  ]
}
```

### Atalhos √öteis no VSCode
- `Ctrl+Shift+G`: Gerar c√≥digo
- `Ctrl+Shift+R`: Refatorar
- `Ctrl+Shift+D`: Documentar
- `Ctrl+Shift+B`: Debug assist

---

## üí° Prompts Especializados para Devs

### Revis√£o de C√≥digo Completa
```
Analise este c√≥digo considerando:

## üîç AN√ÅLISE DE QUALIDADE
- Erros de sintaxe e l√≥gica
- Padr√µes de c√≥digo
- Legibilidade e manutenibilidade
- Estrutura e arquitetura

## üõ°Ô∏è AN√ÅLISE DE SEGURAN√áA
- Vulnerabilidades (OWASP Top 10)
- Valida√ß√£o de entrada
- Autentica√ß√£o/autoriza√ß√£o
- Exposi√ß√£o de dados

## ‚ö° AN√ÅLISE DE PERFORMANCE
- Complexidade algor√≠tmica
- Uso de mem√≥ria
- Opera√ß√µes custosas
- Otimiza√ß√µes poss√≠veis

## üìã SUGEST√ïES DE MELHORIA
Para cada problema identificado, forne√ßa:
- Explica√ß√£o clara
- Impacto potencial
- Solu√ß√£o espec√≠fica com exemplo
- Prioridade (Alta/M√©dia/Baixa)

Organize a resposta estruturada e priorize problemas cr√≠ticos.
```

### Gera√ß√£o de Testes Abrangentes
```
Crie uma suite completa de testes para o c√≥digo selecionado:

## üß™ ESTRUTURA DOS TESTES
- Setup/teardown necess√°rios
- Mocks e stubs apropriados
- Fixtures reutiliz√°veis
- Helpers para testes

## ‚úÖ COBERTURA DE CEN√ÅRIOS
- Happy path principais
- Edge cases e limites
- Error cases e exce√ß√µes
- Boundary testing

## üîÑ TIPOS DE TESTE
- Unit√°rios (fun√ß√µes/m√©todos isolados)
- Integra√ß√£o (intera√ß√£o entre componentes)
- Performance (tempo/mem√≥ria se relevante)
- Regress√£o (bugs conhecidos)

## üìù ORGANIZA√á√ÉO
- Describe/context l√≥gico
- Test names descritivos
- Assertions espec√≠ficas
- Comments para l√≥gica complexa

## üéØ QUALIDADE DOS TESTES
- Independ√™ncia entre testes
- Determinismo consistente
- Velocidade de execu√ß√£o
- Manutenibilidade

Use o framework apropriado e inclua instru√ß√µes de execu√ß√£o.
```

### Refatora√ß√£o Sistem√°tica
```
Refatore o c√≥digo seguindo princ√≠pios de clean code:

## üèóÔ∏è AN√ÅLISE ARQUITETURAL
- SOLID Principles
- Design Patterns aplic√°veis
- Separation of Concerns
- Coupling/Cohesion

## üîß REFATORA√á√ïES ESTRUTURAIS
- Extract Method (funcionalidades complexas)
- Extract Class (responsabilidades distintas)
- Move Method (classes apropriadas)
- Rename (nomes melhores)

## üì¶ ORGANIZA√á√ÉO DE C√ìDIGO
- Modulariza√ß√£o coesa
- Interfaces claras
- Abstra√ß√µes apropriadas
- Dependencies minimizadas

## üéØ MELHORIAS ESPEC√çFICAS
- Error handling robusto
- Logging estruturado
- Configuration externalizada
- Testing facilitado

## üìã PLANO DE REFATORA√á√ÉO
1. Fase 1: Refatora√ß√µes seguras (renomea√ß√£o, extra√ß√£o)
2. Fase 2: Mudan√ßas estruturais (classes, interfaces)
3. Fase 3: Otimiza√ß√µes arquiteturais
4. Valida√ß√£o: Testes para cada fase

Mantenha funcionalidade original e forne√ßa plano de migra√ß√£o gradual.
```

### An√°lise de Imagens T√©cnicas
```
Analise a imagem com foco t√©cnico detalhado:

## üëÅÔ∏è AN√ÅLISE VISUAL GERAL
- Elementos principais visuais
- Layout e estrutura espacial
- Texto vis√≠vel (transcrever e interpretar)
- Cores e design visual

## üíª AN√ÅLISE T√âCNICA ESPEC√çFICA
Se for c√≥digo/diagrama:
- Linguagem/framework identificados
- Padr√µes reconhecidos
- Arquitetura do sistema
- Fluxo de dados

Se for interface/UI:
- UX/UI patterns identificados
- Responsividade avaliada
- Acessibilidade verificada
- Performance visual otimizada

Se for diagrama/arquitetura:
- Componentes identificados
- Relacionamentos analisados
- Fluxos tra√ßados
- Escalabilidade avaliada

## üîç INSIGHTS T√âCNICOS
- Boas pr√°ticas identificadas
- Problemas potenciais apontados
- Melhorias sugeridas espec√≠ficas
- Alternativas propostas

## üìö CONTEXTO E RECOMENDA√á√ïES
- Documenta√ß√£o relevante sugerida
- Ferramentas apropriadas recomendadas
- Pr√≥ximos passos indicados
- Recursos de aprendizado apontados

Forne√ßa insights acion√°veis baseados na an√°lise visual.
```

---

## ‚ö° Otimiza√ß√£o de Performance

### Configura√ß√µes por Hardware

#### GPU Alta Performance
```bash
export OLLAMA_NUM_GPU_LAYERS=50
export OLLAMA_MAX_LOADED_MODELS=3
export OLLAMA_FLASH_ATTENTION=true
```

#### GPU M√©dia
```bash
export OLLAMA_NUM_GPU_LAYERS=35
export OLLAMA_MAX_LOADED_MODELS=2
```

#### CPU/Sistema Limitado
```bash
export OLLAMA_NUM_THREAD=8
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NO_CUDA=1
```

### Otimiza√ß√£o de Mem√≥ria
```bash
# Aumentar swap se necess√°rio
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Verificar uso
free -h
```

### Configura√ß√µes por Modelo
```bash
# Modelos grandes
ollama run codellama-34b --num-gpu-layers 50

# Modelos m√©dios
ollama run deepseek-coder-v2:16b --num-gpu-layers 35

# Modelos pequenos
ollama run phi3 --num-thread 4
```

---

## üîß Solu√ß√£o de Problemas

### Problemas Comuns

#### "Model not found"
```bash
# Verificar nome exato
ollama list

# Baixar novamente
ollama pull <modelo-exato>
```

#### Download Lento
```bash
# Usar mirror alternativo
export OLLAMA_REGISTRY=https://registry.ollama.ai

# Ou configurar proxy
export HTTP_PROXY=http://proxy.company.com:8080
```

#### GPU n√£o Detectada
```bash
# Verificar drivers
nvidia-smi

# Instalar drivers se necess√°rio
sudo apt update && sudo apt install nvidia-driver-XXX
```

#### Mem√≥ria Insuficiente
```bash
# Reduzir camadas GPU
OLLAMA_NUM_GPU_LAYERS=25 ollama run <modelo>

# Usar CPU
OLLAMA_NO_CUDA=1 ollama run <modelo>
```

#### Porta Ocupada
```bash
# Usar porta diferente
ollama serve --port 11435

# Verificar processos
lsof -i :11434
```

### Diagn√≥stico Avan√ßado
```bash
# Ver logs detalhados
ollama serve 2>&1 | tee ollama.log

# Testar conectividade
curl -X POST http://localhost:11434/api/generate -d '{"model": "llama3.1:8b", "prompt": "test"}'

# Verificar GPU
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

---

## üìú Scripts de Automa√ß√£o

### Script de Instala√ß√£o em Lote
```bash
#!/bin/bash
# install_models_batch.sh

MODELS=(
    "llama3.1:8b"
    "deepseek-coder-v2:16b"
    "mistral"
    "llava"
    "phi3"
    "starcoder2:7b"
    "nomic-embed-text"
)

echo "Instalando ${#MODELS[@]} modelos..."

for model in "${MODELS[@]}"; do
    echo "üì• Baixando $model..."
    if ollama pull "$model"; then
        echo "‚úÖ $model instalado com sucesso"
    else
        echo "‚ùå Falha ao instalar $model"
    fi
done

echo "üéâ Instala√ß√£o conclu√≠da!"
echo "Modelos instalados:"
ollama list
```

### Script de Backup e Restaura√ß√£o
```bash
#!/bin/bash
# backup_models.sh

BACKUP_DIR="$HOME/ollama_backup_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "üì¶ Fazendo backup dos modelos..."
cp -r ~/.ollama/models "$BACKUP_DIR/"

echo "‚úÖ Backup conclu√≠do: $BACKUP_DIR"
ls -lh "$BACKUP_DIR/models"
```

```bash
#!/bin/bash
# restore_models.sh

BACKUP_DIR="$1"

if [ -z "$BACKUP_DIR" ]; then
    echo "Uso: $0 <diret√≥rio_de_backup>"
    exit 1
fi

echo "üîÑ Restaurando modelos..."
cp -r "$BACKUP_DIR/models" ~/.ollama/

echo "‚úÖ Restaura√ß√£o conclu√≠da!"
ollama list
```

### Script de Monitoramento
```bash
#!/bin/bash
# monitor_ollama.sh

while true; do
    echo "=== $(date) ==="
    echo "Modelos carregados:"
    ollama ps
    echo ""
    echo "Uso de GPU:"
    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits
    echo ""
    sleep 30
done
```

### Script de Otimiza√ß√£o Autom√°tica
```bash
#!/bin/bash
# optimize_ollama.sh

# Detectar hardware
if command -v nvidia-smi &> /dev/null; then
    GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    
    if [ "$GPU_MEMORY" -gt 8000 ]; then
        export OLLAMA_NUM_GPU_LAYERS=50
        export OLLAMA_MAX_LOADED_MODELS=3
        echo "üéÆ Configurado para GPU alta performance"
    elif [ "$GPU_MEMORY" -gt 4000 ]; then
        export OLLAMA_NUM_GPU_LAYERS=35
        export OLLAMA_MAX_LOADED_MODELS=2
        echo "üíª Configurado para GPU m√©dia"
    else
        export OLLAMA_NUM_GPU_LAYERS=25
        export OLLAMA_MAX_LOADED_MODELS=1
        echo "üñ•Ô∏è Configurado para GPU b√°sica"
    fi
else
    CPU_CORES=$(nproc)
    export OLLAMA_NUM_THREAD=$CPU_CORES
    export OLLAMA_MAX_LOADED_MODELS=1
    export OLLAMA_NO_CUDA=1
    echo "üíæ Configurado para CPU-only ($CPU_CORES cores)"
fi

echo "üöÄ Iniciando Ollama otimizado..."
ollama serve
```

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Modelos Dispon√≠veis](https://ollama.com/library)
- [Open WebUI](https://github.com/open-webui/open-webui)
- [Continue VSCode](https://continue.dev/)

### Tutoriais e Guias
- [Guia de Modelfiles](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- [FAQ Ollama](https://github.com/ollama/ollama/blob/main/docs/faq.md)
- [Otimiza√ß√£o de Performance](https://github.com/ollama/ollama/blob/main/docs/performance.md)

### Comunidades
- [Discord Ollama](https://discord.gg/ollama)
- [Reddit r/ollama](https://reddit.com/r/ollama)
- [Discord Open WebUI](https://discord.gg/open-webui)

### Ferramentas Relacionadas
- [LM Studio](https://lmstudio.ai/) - Alternativa GUI
- [Ollama Web UI](https://github.com/jmorganca/ollama) - Interface web simples
- [Continue](https://continue.dev/) - Extens√£o VSCode

---

## üéØ Recomenda√ß√µes por Caso de Uso

### Desenvolvimento Full-Stack
```bash
ollama pull deepseek-coder-v2:16b  # Principal
ollama pull starcoder2:7b          # Autocomplete
ollama pull llava                  # An√°lise visual
ollama pull nomic-embed-text       # Busca sem√¢ntica
```

### Ci√™ncia de Dados
```bash
ollama pull llama3.1:8b           # An√°lise geral
ollama pull deepseek-coder-v2:16b # C√≥digo Python
ollama pull phi3                  # Racioc√≠nio matem√°tico
ollama pull nomic-embed-text      # Clustering
```

### DevOps/Infrastructure
```bash
ollama pull deepseek-chat         # Scripts e automa√ß√£o
ollama pull codellama-34b         # IaC e configura√ß√µes
ollama pull llava                 # Diagramas de arquitetura
```

### Mobile Development
```bash
ollama pull deepseek-coder-v2:16b # React Native/Flutter
ollama pull starcoder2:7b         # Autocomplete mobile
ollama pull llava                 # UI/UX analysis
```

---

## ‚ùì FAQ

**P: Quanto espa√ßo preciso para uma cole√ß√£o completa?**
R: 50-100GB. Modelos variam de 1GB (tinyllama) a 40GB+ (codellama-34b).

**P: Posso usar m√∫ltiplos modelos simultaneamente?**
R: Sim, mas considere mem√≥ria. Configure `OLLAMA_MAX_LOADED_MODELS`.

**P: Como acelerar downloads?**
R: Use conex√µes r√°pidas, considere mirrors, baixe em hor√°rios de menor tr√°fego.

**P: Modelos funcionam offline?**
R: Sim, ap√≥s download inicial, funcionam completamente offline.

**P: Qual a diferen√ßa entre pull e run?**
R: `pull` baixa o modelo, `run` executa interativamente.

**P: Como atualizar modelos?**
R: Execute `ollama pull <modelo>` novamente para vers√£o mais recente.

**P: Posso criar modelos customizados?**
R: Sim, usando Modelfiles. Veja documenta√ß√£o oficial.

---

## üìû Suporte

Para suporte adicional:
- Consulte documenta√ß√£o oficial
- Abra issues no reposit√≥rio do projeto
- Participe das comunidades Discord/Reddit
- Verifique logs com `ollama serve 2>&1 | tee debug.log`

---

**√öltima atualiza√ß√£o:** $(date)
**Vers√£o do guia:** 2.0
**Compatibilidade:** Ollama v0.1.0+

---

*Este guia foi criado para consolidar todas as informa√ß√µes sobre Ollama discutidas na conversa, incluindo modelos, configura√ß√µes, prompts e solu√ß√µes de problemas. Mantenha-o atualizado conforme novas vers√µes s√£o lan√ßadas.*
