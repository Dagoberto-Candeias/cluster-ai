# üß† Guia de Instala√ß√£o de Modelos Ollama

Este guia fornece instru√ß√µes completas para instalar e gerenciar modelos Ollama no Cluster AI.

## üìã Pr√©-requisitos

Antes de instalar modelos, certifique-se de que:

1. **Ollama est√° instalado**:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Ollama est√° rodando**:
   ```bash
   ollama serve
   ```

3. **Verificar instala√ß√£o**:
   ```bash
   ollama --version
   ```

## üöÄ M√©todos de Instala√ß√£o

### 1. Instala√ß√£o Interativa (Recomendado)

Use o script interativo para sele√ß√£o personalizada:

```bash
# Via script dedicado
./scripts/utils/check_models.sh

# Ou via manager
./manager.sh models
```

**Recursos do menu interativo:**
- ‚úÖ Sele√ß√£o m√∫ltipla de modelos
- ‚úÖ Visualiza√ß√£o de status (instalado/n√£o instalado)
- ‚úÖ Instala√ß√£o em lote
- ‚úÖ Modelos personalizados
- ‚úÖ Confirma√ß√£o antes da instala√ß√£o

### 2. Instala√ß√£o Direta

Para instala√ß√£o r√°pida de modelos espec√≠ficos:

```bash
# Modelo principal recomendado
ollama pull llama3.1:8b

# Modelo de c√≥digo
ollama pull deepseek-coder-v2:16b

# Modelo de vis√£o
ollama pull llava
```

### 3. Instala√ß√£o em Lote

Para instalar m√∫ltiplos modelos de uma vez:

```bash
# Criar script de instala√ß√£o
cat > install_models.sh << 'EOF'
#!/bin/bash
MODELS=(
    "llama3.1:8b"
    "deepseek-coder-v2:16b"
    "mistral"
    "llava"
    "phi3"
)

for model in "${MODELS[@]}"; do
    echo "Instalando $model..."
    ollama pull "$model"
done

echo "Instala√ß√£o conclu√≠da!"
EOF

chmod +x install_models.sh
./install_models.sh
```

## üìö Modelos Recomendados

### Modelos Essenciais

| Modelo | Tamanho | Uso Principal | Comando |
|--------|---------|---------------|---------|
| `llama3.1:8b` | ~5GB | Chat geral, assistente | `ollama pull llama3.1:8b` |
| `deepseek-coder-v2:16b` | ~9GB | Desenvolvimento, c√≥digo | `ollama pull deepseek-coder-v2:16b` |
| `mistral` | ~4GB | Chat avan√ßado | `ollama pull mistral` |

### Modelos por Categoria

#### ü§ñ Modelos de Chat
```bash
ollama pull llama3.1:8b      # Principal
ollama pull llama3.2         # Avan√ßado
ollama pull phi3             # Racioc√≠nio
ollama pull gemma2:9b        # Conversa√ß√£o
ollama pull qwen3            # Performance
```

#### üíª Modelos de C√≥digo
```bash
ollama pull deepseek-coder-v2:16b  # Principal
ollama pull qwen2.5-coder:32b      # Grande
ollama pull starcoder2:7b          # M√©dio
ollama pull deepseek-coder         # Leve
```

#### üëÅÔ∏è Modelos de Vis√£o
```bash
ollama pull llava             # Vis√£o + chat
```

#### üîç Modelos de Embeddings
```bash
ollama pull nomic-embed-text  # Busca sem√¢ntica
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Otimiza√ß√£o para GPU

```bash
# Configurar uso de GPU
export OLLAMA_NUM_GPU_LAYERS=35
export OLLAMA_MAX_LOADED_MODELS=2

# Para sistemas com pouca VRAM
export OLLAMA_NUM_GPU_LAYERS=25
export OLLAMA_MAX_LOADED_MODELS=1
```

### Configura√ß√£o de Servi√ßo (Linux)

```bash
# Editar configura√ß√£o do servi√ßo
sudo systemctl edit ollama

# Adicionar configura√ß√µes
[Service]
Environment="OLLAMA_NUM_GPU_LAYERS=35"
Environment="OLLAMA_MAX_LOADED_MODELS=2"
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

### Configura√ß√£o Personalizada

```bash
# Arquivo de configura√ß√£o personalizado
mkdir -p ~/.ollama
cat > ~/.ollama/config.json << EOF
{
  "num_gpu_layers": 35,
  "max_loaded_models": 2,
  "host": "0.0.0.0",
  "port": "11434"
}
EOF
```

## üìä Gerenciamento de Modelos

### Listar Modelos Instalados

```bash
# Listar todos os modelos
ollama list

# Ver informa√ß√µes detalhadas
ollama show llama3.1:8b
```

### Remover Modelos

```bash
# Remover modelo espec√≠fico
ollama rm llama3.1:8b

# Limpar cache
ollama clean
```

### Verificar Uso de Recursos

```bash
# Ver modelos carregados
ollama ps

# Monitorar uso de GPU
nvidia-smi
```

## üîß Solu√ß√£o de Problemas

### Problema: Download lento

```bash
# Usar mirror alternativo
export OLLAMA_REGISTRY=https://registry.ollama.ai

# Ou configurar proxy
export HTTP_PROXY=http://proxy.company.com:8080
```

### Problema: Falta de espa√ßo

```bash
# Verificar espa√ßo dispon√≠vel
df -h ~/.ollama

# Limpar modelos n√£o usados
ollama ps | awk 'NR>1 {print $1}' | xargs ollama rm
```

### Problema: GPU n√£o detectada

```bash
# Verificar drivers NVIDIA
nvidia-smi

# Instalar drivers se necess√°rio
sudo apt update && sudo apt install nvidia-driver-XXX
```

## üìà Otimiza√ß√£o de Performance

### Para Sistemas com GPU

```bash
# M√°ximo de camadas GPU
export OLLAMA_NUM_GPU_LAYERS=99

# M√∫ltiplos modelos carregados
export OLLAMA_MAX_LOADED_MODELS=3
```

### Para Sistemas CPU-only

```bash
# Otimizar para CPU
export OLLAMA_NUM_THREAD=8
export OLLAMA_MAX_LOADED_MODELS=1
```

### Configura√ß√£o por Modelo

```bash
# Para modelos grandes
ollama run llama3:70b --num-gpu-layers 50

# Para modelos pequenos
ollama run phi3 --num-thread 4
```

## üîÑ Atualiza√ß√£o de Modelos

```bash
# Atualizar modelo espec√≠fico
ollama pull llama3.1:8b  # Baixa vers√£o mais recente

# Verificar atualiza√ß√µes dispon√≠veis
ollama list | grep -E "(llama3|mistral|phi)" | sort
```

## üìù Scripts de Automa√ß√£o

### Script de Backup de Modelos

```bash
#!/bin/bash
# backup_models.sh

BACKUP_DIR="$HOME/ollama_models_backup_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Fazendo backup dos modelos Ollama..."
cp -r ~/.ollama/models "$BACKUP_DIR/"

echo "Backup conclu√≠do: $BACKUP_DIR"
```

### Script de Restaura√ß√£o

```bash
#!/bin/bash
# restore_models.sh

BACKUP_DIR="$1"

if [ -z "$BACKUP_DIR" ]; then
    echo "Uso: $0 <diret√≥rio_de_backup>"
    exit 1
fi

echo "Restaurando modelos..."
cp -r "$BACKUP_DIR/models" ~/.ollama/

echo "Restaura√ß√£o conclu√≠da!"
```

## üéØ Recomenda√ß√µes por Caso de Uso

### Desenvolvimento de Software
```bash
ollama pull deepseek-coder-v2:16b
ollama pull qwen2.5-coder:32b
ollama pull starcoder2:7b
```

### Chat e Assist√™ncia Geral
```bash
ollama pull llama3.1:8b
ollama pull mistral
ollama pull gemma2:9b
```

### An√°lise de Imagens
```bash
ollama pull llava
ollama pull llama3.1:8b  # Para descri√ß√£o
```

### Processamento de Linguagem Natural
```bash
ollama pull llama3.1:8b
ollama pull nomic-embed-text
ollama pull phi3
```

## üìö Recursos Adicionais

- [Documenta√ß√£o Oficial Ollama](https://github.com/ollama/ollama)
- [Modelos Dispon√≠veis](https://ollama.com/library)
- [Guia de Otimiza√ß√£o](https://github.com/ollama/ollama/blob/main/docs/faq.md)
- [Configura√ß√µes Avan√ßadas](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

---

## ‚ùì FAQ

**P: Quanto espa√ßo em disco preciso?**
R: Modelos variam de 1GB (phi3) a 40GB+ (modelos grandes). Planeje 50-100GB para uma cole√ß√£o completa.

**P: Posso usar m√∫ltiplos modelos simultaneamente?**
R: Sim, mas considere a mem√≥ria dispon√≠vel. Use `OLLAMA_MAX_LOADED_MODELS` para controlar.

**P: Como acelerar downloads?**
R: Use conex√µes r√°pidas, considere mirrors alternativos, ou baixe durante hor√°rios de menor tr√°fego.

**P: Modelos funcionam offline?**
R: Sim, ap√≥s o download inicial, todos os modelos funcionam completamente offline.

Para suporte adicional, consulte a documenta√ß√£o do Cluster AI ou abra uma issue no reposit√≥rio.
