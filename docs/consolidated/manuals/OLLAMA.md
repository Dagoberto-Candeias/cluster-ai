# ðŸ¤– Manual Completo do Ollama - Modelos de IA Local

Guia completo para instalaÃ§Ã£o, configuraÃ§Ã£o e uso dos modelos Ollama no Cluster AI.

## ðŸ“‹ Ãndice
- [ðŸ¤– Manual Completo do Ollama - Modelos de IA Local](#-manual-completo-do-ollama---modelos-de-ia-local)
  - [ðŸ“‹ Ãndice](#-Ã­ndice)
  - [ðŸŽ¯ IntroduÃ§Ã£o](#-introduÃ§Ã£o)
    - [O que Ã© o Ollama?](#o-que-Ã©-o-ollama)
    - [Casos de Uso no Cluster AI](#casos-de-uso-no-cluster-ai)
  - [ðŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#-instalaÃ§Ã£o-e-configuraÃ§Ã£o)
    - [InstalaÃ§Ã£o AutomÃ¡tica](#instalaÃ§Ã£o-automÃ¡tica)
    - [InstalaÃ§Ã£o Manual](#instalaÃ§Ã£o-manual)
    - [ConfiguraÃ§Ã£o BÃ¡sica](#configuraÃ§Ã£o-bÃ¡sica)
    - [VerificaÃ§Ã£o da InstalaÃ§Ã£o](#verificaÃ§Ã£o-da-instalaÃ§Ã£o)
  - [ðŸ“Š Modelos DisponÃ­veis](#-modelos-disponÃ­veis)
    - [ðŸ“ Modelos de Chat (ConversaÃ§Ã£o)](#-modelos-de-chat-conversaÃ§Ã£o)
    - [ðŸ’» Modelos de CodificaÃ§Ã£o](#-modelos-de-codificaÃ§Ã£o)

## ðŸŽ¯ IntroduÃ§Ã£o

### O que Ã© o Ollama?
O Ollama Ã© uma plataforma para executar modelos de linguagem grandes (LLMs) localmente, oferecendo:

- âœ… **ExecuÃ§Ã£o Local**: Sem dependÃªncia de internet apÃ³s download
- âœ… **Multi-modelos**: Suporte a diversos modelos de IA
- âœ… **OtimizaÃ§Ã£o**: ConfiguraÃ§Ã£o automÃ¡tica para CPU/GPU
- âœ… **API REST**: Interface padrÃ£o para integraÃ§Ã£o

### Casos de Uso no Cluster AI
- **AssistÃªncia Ã  ProgramaÃ§Ã£o**: GeraÃ§Ã£o e revisÃ£o de cÃ³digo
- **AnÃ¡lise de Dados**: Processamento e interpretaÃ§Ã£o de textos
- **Chat e Q&A**: Interface conversacional com modelos
- **Processamento DistribuÃ­do**: IntegraÃ§Ã£o com Dask para tasks em paralelo

## ðŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### InstalaÃ§Ã£o AutomÃ¡tica
```bash
# Via script do Cluster AI (recomendado)
./install_cluster.sh

# O script automaticamente:
# 1. Instala o Ollama
# 2. Configura o serviÃ§o
# 3. Baixa modelos essenciais
# 4. Otimiza para hardware disponÃ­vel
```

### InstalaÃ§Ã£o Manual
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Configurar serviÃ§o
sudo systemctl enable ollama
sudo systemctl start ollama

# Verificar instalaÃ§Ã£o
ollama --version
```

### ConfiguraÃ§Ã£o BÃ¡sica
```bash
# Configurar para aceitar conexÃµes externas
sudo mkdir -p /etc/systemd/system/ollama.service.d/
sudo tee /etc/systemd/system/ollama.service.d/environment.conf > /dev/null << EOL
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_NUM_GPU_LAYERS=35"
EOL

sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### VerificaÃ§Ã£o da InstalaÃ§Ã£o
```bash
# Verificar se o serviÃ§o estÃ¡ rodando
systemctl status ollama

# Testar API
curl http://localhost:11434/api/tags

# Verificar modelos instalados
ollama list
```

## ðŸ“Š Modelos DisponÃ­veis

### ðŸ“ Modelos de Chat (ConversaÃ§Ã£o)

| Modelo | Tamanho | Empresa | Melhor para | Comando |
|--------|---------|---------|-------------|---------|
| **llama3:8b** | 8B | Meta | Chat geral | `ollama pull llama3:8b` |
| **llama3.1:8b** | 8B | Meta | DiÃ¡logos complexos | `ollama pull llama3.1:8b` |
| **phi3** | 3.8B | Microsoft | RaciocÃ­nio lÃ³gico | `ollama pull phi3` |
| **mistral** | 7B | Mistral AI | Respostas rÃ¡pidas | `ollama pull mistral` |
| **deepseek-chat** | 16B | DeepSeek | AssistÃªncia tÃ©cnica | `ollama pull deepseek-chat` |
| **gemma2:9b** | 9B | Google | Conversas multi-turno | `ollama pull gemma2:9b` |
| **qwen3** | 14B | Alibaba | Q&A longo | `ollama pull qwen3` |
| **llama2** | 7B | Meta | NLP tradicional | `ollama pull llama2` |
| **vicuna-13b** | 13B | LMSys | DiÃ¡logo natural | `ollama pull vicuna-13b` |
| **chatglm-6b** | 6B | THUDM | ChinÃªs/InglÃªs | `ollama pull chatglm-6b` |

### ðŸ’» Modelos de CodificaÃ§Ã£o

| Modelo | Linguagens | Casos de Uso | Comando |
|--------|------------|-------------|---------|
| **deepseek-coder-v2:16b** | 30+ | Debug, autocompletar | `ollama pull deepseek-coder-v2:16b` |
| **starcoder2:7b** | 80+ | SugestÃµes avanÃ§adas | `ollama pull starcoder2:7b` |
| **codellama-34b** | Python/JS | RefatoraÃ§Ã£o | `ollama pull codellama-34b` |
| **qwen2.5-coder:1.5b** | Python | Autocomplete rÃ¡pido | `ollama pull qwen2.5-coder:1.5b` |
| **mpt-30b-code** | 20+ | GeraÃ§Ã£o complexa | `ollama pull mpt-30b-code` |

### ðŸ–¼ï¸ Modelos Multimodais (Imagem + Texto)

| Modelo | Recursos | Casos de Uso | Comando |
|--------|----------|-------------|---------|
| **llava** | Imagens | AnÃ¡lise visual | `ollama pull llava` |
| **codegemma** | CÃ³digo + Imagens | Diagramas tÃ©cnicos | `ollama pull codegemma` |
| **qwen2-vl** | Multimodal avanÃ§ado | VisÃ£o computacional | `ollama pull qwen2-vl` |
| **deepseek-vision** | VisÃ£o computacional | AnÃ¡lise tÃ©cnica | `ollama pull deepseek-vision` |

### ðŸ”¤ Modelos de Embeddings

| Modelo | DimensÃµes | Casos de Uso | Comando |
|--------|-----------|-------------|---------|
| **nomic-embed-text** | 768 | Busca semÃ¢ntica | `ollama pull nomic-embed-text` |
| **text-embedding-3-large** | 3072 | NLP avanÃ§ado | `ollama pull text-embedding-3-large` |
| **gemma-embed** | 1024 | Embeddings eficientes | `ollama pull gemma-embed` |
| **mistral-embed** | 4096 | ClusterizaÃ§Ã£o | `ollama pull mistral-embed` |

### Download de Modelos
```bash
# Download individual
ollama pull llama3:8b
ollama pull deepseek-coder-v2:16b

# Download em lote (background)
for model in llama3:8b mistral deepseek-coder; do
    ollama pull $model &
done

# Verificar progresso
ollama list

# Listar modelos disponÃ­veis
ollama list
```

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### ConfiguraÃ§Ã£o de GPU
```bash
# Detectar tipo de GPU
lspci | grep -i nvidia && echo "NVIDIA detectada" || echo "NVIDIA nÃ£o detectada"
lspci | grep -i amd/ati && echo "AMD detectada" || echo "AMD nÃ£o detectada"

# ConfiguraÃ§Ã£o automÃ¡tica no Cluster AI
# O script detecta automaticamente e configura:
# - NVIDIA: AtÃ© 35 camadas GPU
# - AMD: AtÃ© 20 camadas GPU  
# - CPU-only: ConfiguraÃ§Ã£o otimizada
```

### ConfiguraÃ§Ã£o Manual de GPU
```bash
# Criar diretÃ³rio de configuraÃ§Ã£o
mkdir -p ~/.ollama

# ConfiguraÃ§Ã£o para NVIDIA
cat > ~/.ollama/config.json << 'EOL'
{
    "runners": {
        "nvidia": {
            "url": "https://github.com/ollama/ollama/blob/main/gpu/nvidia/runner.cu"
        }
    },
    "environment": {
        "OLLAMA_NUM_GPU_LAYERS": "35",
        "OLLAMA_MAX_LOADED_MODELS": "3",
        "OLLAMA_KEEP_ALIVE": "24h"
    }
}
EOL

# ConfiguraÃ§Ã£o para AMD
cat > ~/.ollama/config.json << 'EOL'
{
    "runners": {
        "rocm": {
            "url": "https://github.com/ollama/ollama/blob/main/gpu/rocm/runner.cc"
        }
    },
    "environment": {
        "OLLAMA_NUM_GPU_LAYERS": "20",
        "OLLAMA_MAX_LOADED_MODELS": "2"
    }
}
EOL

# ConfiguraÃ§Ã£o CPU-only
cat > ~/.ollama/config.json << 'EOL'
{
    "environment": {
        "OLLAMA_MAX_LOADED_MODELS": "1",
        "OLLAMA_KEEP_ALIVE": "1h"
    }
}
EOL

# Reiniciar serviÃ§o
sudo systemctl restart ollama
```

### ParÃ¢metros de ExecuÃ§Ã£o
```bash
# Executar com parÃ¢metros especÃ­ficos
ollama run llama3:8b --temperature 0.7 --max-tokens 1024

# ParÃ¢metros comuns:
# --temperature: 0.0-1.0 (criatividade)
# --max-tokens: 1-4096 (tamanho da resposta)
# --top-p: 0.0-1.0 (diversidade)
# --top-k: 1-100 (seleÃ§Ã£o de tokens)
```

### ConfiguraÃ§Ã£o de MemÃ³ria
```bash
# Limitar uso de memÃ³ria por modelo
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_NUM_PARALLEL=1

# Para modelos grandes, ajustar memÃ³ria
export OLLAMA_MMAP=1  # Memory mapping para modelos grandes
```

## ðŸ IntegraÃ§Ã£o com Python

### API BÃ¡sica
```python
import requests
import json

def ask_ollama(prompt, model="llama3", host="localhost", port=11434):
    """
    Consulta o Ollama via API
    """
    url = f"http://{host}:{port}/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "max_tokens": 1024
        }
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        return response.json()['response']
    except requests.exceptions.RequestException as e:
        return f"Erro: {str(e)}"

# Exemplo de uso
resposta = ask_ollama("Explique o que Ã© machine learning")
print(resposta)
```

### API com Streaming
```python
def ask_ollama_stream(prompt, model="llama3", callback=None):
    """
    Consulta com streaming para respostas longas
    """
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    full_response = ""
    
    try:
        response = requests.post(url, json=payload, stream=True, timeout=60)
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if 'response' in data:
                    chunk = data['response']
                    full_response += chunk
                    if callback:
                        callback(chunk)
                
                if data.get('done', False):
                    break
                    
        return full_response
        
    except Exception as e:
        return f"Erro: {str(e)}"

# Exemplo com callback
def print_chunk(chunk):
    print(chunk, end='', flush=True)

resposta = ask_ollama_stream("Explique IA", callback=print_chunk)
```

### Biblioteca Ollama Python
```python
# Instalar biblioteca oficial
# pip install ollama

import ollama

# Uso simples
response = ollama.chat(
    model='llama3',
    messages=[{'role': 'user', 'content': 'Por que o cÃ©u Ã© azul?'}]
)
print(response['message']['content'])

# Com opÃ§Ãµes
response = ollama.chat(
    model='llama3',
    messages=[{'role': 'user', 'content': 'Explique quantum computing'}],
    options={
        'temperature': 0.3,
        'max_tokens': 512
    }
)

# Gerar embeddings
response = ollama.embeddings(
    model='nomic-embed-text',
    prompt='Texto para embed'
)
embeddings = response['embeddings']
```

### IntegraÃ§Ã£o com Dask (Processamento DistribuÃ­do)
```python
from dask.distributed import Client
import dask.bag as db

def analyze_with_ollama(text):
    """
    FunÃ§Ã£o para processamento distribuÃ­do com Ollama
    """
    prompt = f"""
    Analise o texto abaixo e retorne JSON com:
    - sentimento (POSITIVO/NEGATIVO/NEUTRO)
    - tÃ³picos principais (lista)
    - resumo (mÃ¡ximo 50 palavras)
    
    Texto: {text}
    """
    
    try:
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': 'llama3',
                'prompt': prompt,
                'stream': False,
                'format': 'json'
            },
            timeout=30
        )
        return response.json()['response']
    except:
        return "{}"

# Processamento paralelo
texts = ["Texto 1", "Texto 2", "Texto 3"]  # Sua lista de textos
bag = db.from_sequence(texts, npartitions=2)
results = bag.map(analyze_with_ollama).compute()

for text, result in zip(texts, results):
    print(f"Texto: {text}")
    print(f"AnÃ¡lise: {result}")
    print("-" * 50)
```

## ðŸ”§ OtimizaÃ§Ã£o de Performance

### OtimizaÃ§Ã£o para Hardware
```bash
# Verificar hardware disponÃ­vel
nvidia-smi  # Para NVIDIA
rocm-smi    # Para AMD
lscpu       # InformaÃ§Ãµes da CPU
free -h     # MemÃ³ria disponÃ­vel

# Ajustar nÃºmero de camadas GPU
# (Quanto mais camadas na GPU, melhor a performance)
export OLLAMA_NUM_GPU_LAYERS=35  # NVIDIA high-end
export OLLAMA_NUM_GPU_LAYERS=20  # NVIDIA mid-range  
export OLLAMA_NUM_GPU_LAYERS=10  # NVIDIA entry-level
export OLLAMA_NUM_GPU_LAYERS=8   # AMD
```

### Gerenciamento de MemÃ³ria
```bash
# Limitar modelos em memÃ³ria
export OLLAMA_MAX_LOADED_MODELS=2

# Para sistemas com pouca RAM
export OLLAMA_MMAP=1      # Memory mapping
export OLLAMA_F16=0       # Usar float32 (mais preciso, mais memÃ³ria)
export OLLAMA_KEEP_ALIVE="30m"  # Tempo para manter modelo na memÃ³ria
```

### Benchmark de Performance
```python
import time
import requests

def benchmark_ollama(model="llama3", prompt="Hello", repetitions=5):
    """
    Teste de performance do Ollama
    """
    times = []
    
    for i in range(repetitions):
        start_time = time.time()
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={'model': model, 'prompt': prompt, 'stream': False}
        )
        
        end_time = time.time()
        times.append(end_time - start_time)
        
        if response.status_code != 200:
            print(f"Erro na iteraÃ§Ã£o {i+1}")
            continue
    
    avg_time = sum(times) / len(times)
    tokens_per_second = len(response.json()['response'].split()) / avg_time
    
    print(f"Modelo: {model}")
    print(f"Tempo mÃ©dio: {avg_time:.2f}s")
    print(f"Tokens por segundo: {tokens_per_second:.2f}")
    print(f"LatÃªncia mÃ­nima: {min(times):.2f}s")
    print(f"LatÃªncia mÃ¡xima: {max(times):.2f}s")
    
    return times

# Executar benchmark
benchmark_ollama("llama3", "Explique machine learning em 50 palavras")
```

## ðŸš¨ SoluÃ§Ã£o de Problemas

### Problemas Comuns

#### Ollama NÃ£o Inicia
```bash
# Verificar status
systemctl status ollama

# Verificar logs
journalctl -u ollama -f

# Reiniciar serviÃ§o
sudo systemctl restart ollama

# Verificar portas
sudo lsof -i :11434
```

#### Modelos NÃ£o Carregam
```bash
# Verificar espaÃ§o em disco
df -h

# Limpar cache
ollama rm --all
ollama pull llama3:8b

# Verificar permissÃµes
ls -la ~/.ollama/
```

#### Performance Ruim
```bash
# Verificar uso GPU
nvidia-smi

# Verificar uso CPU
top

# Verificar uso memÃ³ria
free -h

# Reduzir camadas GPU (se muita memÃ³ria)
export OLLAMA_NUM_GPU_LAYERS=20
sudo systemctl restart ollama
```

#### Erro de CUDA/GPU
```bash
# Verificar drivers NVIDIA
nvidia-smi

# Verificar CUDA
nvcc --version

# Instalar drivers (Ubuntu)
sudo apt install nvidia-driver-535 nvidia-cuda-toolkit

# ForÃ§ar modo CPU
export OLLAMA_NUM_GPU_LAYERS=0
sudo systemctl restart ollama
```

### Comandos de DiagnÃ³stico
```bash
# Health check completo
curl http://localhost:11434/api/tags
curl http://localhost:11434/api/version

# Verificar modelos
ollama list
ollama ps

# Verificar informaÃ§Ãµes do sistema
ollama info

# Limpar cache
ollama rm --all
ollama pull llama3:8b
```

### Logs Detalhados
```bash
# Modo verbose
OLLAMA_DEBUG=1 ollama serve

# Logs do sistema
journalctl -u ollama -f -n 100

# Logs especÃ­ficos
sudo tail -f /var/log/syslog | grep ollama
```

## ðŸŽ¯ Exemplos PrÃ¡ticos

### Assistente de CÃ³digo
```python
def ask_code_assistant(code, task="explain", model="deepseek-coder"):
    """
    Assistente para anÃ¡lise de cÃ³digo
    """
    prompts = {
        "explain": f"Explique este cÃ³digo:\n\n{code}",
        "debug": f"Encontre bugs neste cÃ³digo:\n\n{code}",
        "optimize": f"Otimize este cÃ³digo:\n\n{code}",
        "document": f"Documente este cÃ³digo:\n\n{code}"
    }
    
    prompt = prompts.get(task, prompts["explain"])
    
    response = requests.post(
        'http://localhost:11434/api/generate',
        json={
            'model': model,
            'prompt': prompt,
            'stream': False,
            'options': {'temperature': 0.3}
        }
    )
    
    return response.json()['response']

# Exemplo
code = """
def calculate_sum(numbers):
    total = 0
    for num in numbers:
        total = total + num
    return total
"""

explicacao = ask_code_assistant(code, "explain")
print(explicacao)
```

### AnÃ¡lise de Sentimento em Lote
```python
from dask.distributed import Client
import dask.bag as db

def analyze_sentiment_batch(texts, model="llama3"):
    """
    AnÃ¡lise de sentimento distribuÃ­da
    """
    def analyze_single(text):
        prompt = f"""
        Analise o sentimento do texto abaixo.
        Responda apenas com: POSITIVO, NEGATIVO ou NEUTRO.
        
        Texto: {text}
        """
        
        try:
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': model,
                    'prompt': prompt,
                    'stream': False,
                    'options': {'temperature': 0.1}
                },
                timeout=10
            )
            return response.json()['response'].strip()
        except:
            return "ERROR"
    
    # Processamento paralelo com Dask
    client = Client('localhost:8786')
    bag = db.from_sequence(texts, npartitions=4)
    results = bag.map(analyze_single).compute()
    client.close()
    
    return results

# Exemplo
texts = [
    "Adorei este produto! Entrega rÃ¡pida e qualidade excelente.",
    "PÃ©ssimo atendimento, nunca mais compro aqui.",
    "Produto conforme descrito, entrega no prazo.",
    "Qualidade inferior ao esperado para o preÃ§o pago."
]

sentimentos = analyze_sentiment_batch(texts)
for texto, sentimento in zip(texts, sentimentos):
    print(f"{sentimento}: {texto[:50]}...")
```

### Chat Persistente com HistÃ³rico
```python
class ChatSession:
    def __init__(self, model="llama3"):
        self.model = model
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def ask(self, prompt):
        self.add_message("user", prompt)
        
        response = requests.post(
            'http://localhost:11434/api/chat',
            json={
                'model': self.model,
                'messages': self.history,
                'stream': False
            }
        )
        
        assistant_response = response.json()['message']['content']
        self.add_message("assistant", assistant_response)
        
        return assistant_response
    
    def clear_history(self):
        self.history = []

# Exemplo de uso
chat = ChatSession("llama3")
print(chat.ask("OlÃ¡! Como vocÃª pode me ajudar?"))
print(chat.ask("Me explique sobre machine learning"))
print(chat.ask("Agora fale sobre deep learning"))
```

### GeraÃ§Ã£o de ConteÃºdo
```python
def generate_content(topic, style="formal", length="short", model="llama3"):
    """
    GeraÃ§Ã£o de conteÃºdo com parÃ¢metros controlados
    """
    length_map = {
        "short": "50 palavras",
        "medium": "150 palavras", 
        "long": "300 palavras"
    }
    
    prompt = f"""
    Escreva um texto {style} sobre {topic}.
    O texto deve ter aproximadamente {length_map[length]}.
    Mantenha o conteÃºdo informativo e bem estruturado.
    """
    
    response = requests.post(
        'http://localhost:11434/api/generate',
        json={
            'model': model,
            'prompt': prompt,
            'stream': False,
            'options': {
                'temperature': 0.7 if style == "creative" else 0.3,
                'max_tokens': 500 if length == "long" else 200
            }
        }
    )
    
    return response.json()['response']

# Exemplos
texto_curto = generate_content("inteligÃªncia artificial", "formal", "short")
texto_longo = generate_content("histÃ³ria da computaÃ§Ã£o", "creative", "long")
```

---

**ðŸŽ¯ PrÃ³ximos Passos**:
- Explore [IntegraÃ§Ã£o com OpenWebUI](../manuals/OPENWEBUI.md)
- Configure [Backup de Modelos](../manuals/BACKUP.md)
- Otimize para [ProduÃ§Ã£o](../../deployments/production/README.md)

