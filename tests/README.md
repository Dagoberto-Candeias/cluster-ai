# ðŸ§ª SuÃ­te de Testes - Cluster AI

Esta Ã© uma suÃ­te completa de testes para o projeto **Cluster AI**, projetada para garantir qualidade, confiabilidade e manutenibilidade do cÃ³digo.

## ðŸ“‹ VisÃ£o Geral

A suÃ­te de testes estÃ¡ organizada em diferentes tipos de testes:

- **UnitÃ¡rios**: Testam funÃ§Ãµes individuais
- **IntegraÃ§Ã£o**: Testam interaÃ§Ã£o entre componentes
- **End-to-End**: Testam fluxos completos
- **Performance**: Testam desempenho e recursos
- **SeguranÃ§a**: Testam vulnerabilidades e permissÃµes
- **Bash**: Testam scripts shell

## ðŸš€ Executando os Testes

### MÃ©todo 1: Executor Principal (Recomendado)

Use o script principal para executar todos os testes:

```bash
# Executar todos os testes
python tests/run_all_tests.py

# Executar apenas testes unitÃ¡rios
python tests/run_all_tests.py --unit

# Executar apenas testes de integraÃ§Ã£o
python tests/run_all_tests.py --integration

# Executar apenas testes end-to-end
python tests/run_all_tests.py --e2e

# Executar apenas testes de performance
python tests/run_all_tests.py --performance

# Executar apenas testes de seguranÃ§a
python tests/run_all_tests.py --security

# Executar apenas testes Bash
python tests/run_all_tests.py --bash

# Executar apenas linting
python tests/run_all_tests.py --lint

# OpÃ§Ãµes adicionais
python tests/run_all_tests.py --fail-fast    # Parar no primeiro erro
python tests/run_all_tests.py --parallel     # Executar em paralelo
python tests/run_all_tests.py --no-lint      # Pular linting
python tests/run_all_tests.py --no-coverage  # Pular anÃ¡lise de cobertura
```

### MÃ©todo 2: pytest Direto

Para execuÃ§Ã£o mais granular com pytest:

```bash
# Todos os testes
pytest

# Testes unitÃ¡rios especÃ­ficos
pytest tests/unit/test_demo_cluster.py -v

# Testes de integraÃ§Ã£o
pytest tests/integration/ -v

# Com cobertura
pytest --cov=. --cov-report=html

# Testes especÃ­ficos por marcador
pytest -m unit
pytest -m integration
pytest -m e2e
pytest -m performance
pytest -m security
```

### MÃ©todo 3: Scripts Bash

Para testes de scripts Bash (requer BATS):

```bash
# Instalar BATS
sudo apt-get install bats

# Executar testes Bash
bats tests/bash/
```

## ðŸ“ Estrutura dos Testes

```
tests/
â”œâ”€â”€ conftest.py              # ConfiguraÃ§Ãµes globais
â”œâ”€â”€ pytest.ini              # ConfiguraÃ§Ãµes do pytest
â”œâ”€â”€ test_requirements.txt   # DependÃªncias de teste
â”œâ”€â”€ run_all_tests.py        # Executor principal
â”œâ”€â”€ README.md               # Esta documentaÃ§Ã£o
â”‚
â”œâ”€â”€ unit/                   # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ test_demo_cluster.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ integration/            # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ test_dask_integration.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ e2e/                    # Testes end-to-end
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ performance/            # Testes de performance
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ security/               # Testes de seguranÃ§a
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ bash/                   # Testes de scripts Bash
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ fixtures/               # Dados de teste
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ reports/                # RelatÃ³rios gerados
    â”œâ”€â”€ coverage/
    â”œâ”€â”€ unit_tests.html
    â”œâ”€â”€ integration_tests.html
    â””â”€â”€ ...
```

## ðŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente

### DependÃªncias Python

Instale as dependÃªncias de teste:

```bash
pip install -r tests/test_requirements.txt
```

### DependÃªncias do Sistema

Para Ubuntu/Debian:

```bash
sudo apt-get update
sudo apt-get install -y \
    curl \
    wget \
    git \
    build-essential \
    python3-dev \
    bats  # Para testes Bash
```

### VariÃ¡veis de Ambiente

Configure variÃ¡veis de ambiente para testes:

```bash
export CLUSTER_AI_TEST_MODE=1
export CLUSTER_AI_CONFIG_DIR=/tmp/test_config
export CLUSTER_AI_LOG_DIR=/tmp/test_logs
```

## ðŸ“Š RelatÃ³rios e Cobertura

### RelatÃ³rios HTML

Os testes geram relatÃ³rios HTML automaticamente:

- `tests/reports/unit_tests.html` - RelatÃ³rio de testes unitÃ¡rios
- `tests/reports/integration_tests.html` - RelatÃ³rio de testes de integraÃ§Ã£o
- `tests/reports/coverage/index.html` - RelatÃ³rio de cobertura

### Cobertura de CÃ³digo

A cobertura mÃ­nima requerida Ã© de **80%**. Para verificar:

```bash
pytest --cov=. --cov-report=term-missing --cov-fail-under=80
```

### RelatÃ³rio de Performance

Para testes de performance:

```bash
pytest tests/performance/ --benchmark-only --benchmark-json=benchmark.json
```

## ðŸ”§ Desenvolvimento de Testes

### Estrutura BÃ¡sica de um Teste

```python
import pytest
from unittest.mock import MagicMock, patch

class TestMinhaClasse:
    """Testes para MinhaClasse"""

    def test_metodo_basico(self):
        """Testa funcionalidade bÃ¡sica"""
        # Arrange
        obj = MinhaClasse()

        # Act
        result = obj.metodo()

        # Assert
        assert result == esperado

    @patch('modulo.funcao_externa')
    def test_metodo_com_mock(self, mock_funcao):
        """Testa mÃ©todo com dependÃªncia externa mockada"""
        # Arrange
        mock_funcao.return_value = "mocked_result"
        obj = MinhaClasse()

        # Act
        result = obj.metodo_que_usa_externa()

        # Assert
        assert result == "mocked_result"
        mock_funcao.assert_called_once()

    @pytest.mark.parametrize("input,expected", [
        (1, 2),
        (2, 4),
        (3, 6),
    ])
    def test_metodo_parametrizado(self, input, expected):
        """Testa mÃ©todo com mÃºltiplos parÃ¢metros"""
        assert MinhaClasse.metodo_estatico(input) == expected
```

### Fixtures Personalizadas

Use fixtures para compartilhar configuraÃ§Ã£o:

```python
@pytest.fixture
def meu_fixture():
    """Fixture personalizado"""
    # Setup
    resource = criar_recurso()

    yield resource

    # Teardown
    resource.cleanup()

def test_usa_fixture(meu_fixture):
    """Teste que usa fixture"""
    assert meu_fixture.esta_pronto()
```

### Mocks e Stubs

Para dependÃªncias externas:

```python
@patch('requests.get')
def test_api_call(mock_get):
    """Testa chamada de API"""
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"data": "test"}
    mock_get.return_value = mock_response

    result = minha_funcao_que_faz_request()

    assert result["data"] == "test"
```

## ðŸ·ï¸ Marcadores de Teste

### Marcadores DisponÃ­veis

- `@pytest.mark.unit` - Testes unitÃ¡rios
- `@pytest.mark.integration` - Testes de integraÃ§Ã£o
- `@pytest.mark.e2e` - Testes end-to-end
- `@pytest.mark.performance` - Testes de performance
- `@pytest.mark.security` - Testes de seguranÃ§a
- `@pytest.mark.slow` - Testes que demoram mais
- `@pytest.mark.skip_ci` - Pular em CI/CD

### Usando Marcadores

```python
@pytest.mark.unit
def test_rapido():
    """Teste unitÃ¡rio rÃ¡pido"""
    pass

@pytest.mark.slow
def test_lento():
    """Teste que demora muito"""
    time.sleep(10)

@pytest.mark.skip_ci
def test_apenas_local():
    """Teste que nÃ£o roda no CI"""
    pass
```

## ðŸ”’ Testes de SeguranÃ§a

### Executando Testes de SeguranÃ§a

```bash
# Com Bandit
bandit -r . -f json -o security_report.json

# Com Safety
safety check --output json
```

### Tipos de Testes de SeguranÃ§a

- ValidaÃ§Ã£o de permissÃµes de arquivo
- VerificaÃ§Ã£o de configuraÃ§Ãµes seguras
- Testes de injeÃ§Ã£o
- ValidaÃ§Ã£o de entrada/saÃ­da

## ðŸ“ˆ Testes de Performance

### Benchmarks

```python
import pytest_benchmark

def test_performance(benchmark):
    """Teste de performance com benchmark"""
    result = benchmark(minha_funcao_custosa, arg1, arg2)
    assert result is not None
```

### Profiling

```python
import cProfile
import pstats

def test_with_profiling():
    """Teste com profiling"""
    profiler = cProfile.Profile()
    profiler.enable()

    # CÃ³digo a ser perfilado
    minha_funcao()

    profiler.disable()
    stats = pstats.Stats(profiler).sort_stats('cumulative')
    stats.print_stats()
```

## ðŸ› Debugging de Testes

### Executando Testes em Debug

```bash
# Parar no primeiro erro
pytest --pdb

# Modo verbose
pytest -v -s

# Executar teste especÃ­fico
pytest tests/unit/test_demo_cluster.py::TestFibonacciTask::test_fibonacci_base_cases -v -s
```

### Logs de Debug

```python
import logging

def test_com_logs(caplog):
    """Teste que captura logs"""
    caplog.set_level(logging.DEBUG)

    minha_funcao_que_loga()

    assert "mensagem esperada" in caplog.text
```

## ðŸ“‹ CI/CD Integration

### GitHub Actions

Os testes sÃ£o executados automaticamente via GitHub Actions:

- **Push/PR**: Executa todos os testes
- **Schedule**: Testes diÃ¡rios de regressÃ£o
- **Release**: Testes completos antes do deploy

### ConfiguraÃ§Ã£o Local de CI

Para simular CI localmente:

```bash
# Executar como no CI
python tests/run_all_tests.py --fail-fast --parallel

# Com variÃ¡veis de ambiente do CI
CI=true python tests/run_all_tests.py
```

## ðŸŽ¯ Melhores PrÃ¡ticas

### PrincÃ­pios Gerais

1. **Teste uma coisa por vez** - Cada teste deve validar um comportamento especÃ­fico
2. **Nome descritivo** - O nome do teste deve explicar o que estÃ¡ sendo testado
3. **IndependÃªncia** - Testes nÃ£o devem depender uns dos outros
4. **Rapidez** - Testes devem ser rÃ¡pidos para executar frequentemente
5. **Confiabilidade** - Testes devem passar consistentemente

### Estrutura de Teste

```python
def test_descricao_clara_do_que_esta_sendo_testado():
    # Given - Contexto/Setup
    setup_dados = criar_dados_de_teste()

    # When - AÃ§Ã£o
    resultado = funcao_sob_teste(setup_dados)

    # Then - VerificaÃ§Ã£o
    assert resultado == esperado
    assert condicao_adicional
```

### Cobertura de CenÃ¡rios

- **Caminho feliz** - Funcionamento normal
- **Casos extremos** - Valores limites, vazios, nulos
- **CenÃ¡rios de erro** - ExceÃ§Ãµes, falhas, timeouts
- **Edge cases** - CondiÃ§Ãµes especiais

## ðŸ“ž Suporte e ContribuiÃ§Ã£o

### Relatando Problemas

Para reportar problemas nos testes:

1. Verifique se o problema jÃ¡ foi reportado
2. Crie um issue detalhado com:
   - DescriÃ§Ã£o do problema
   - Passos para reproduzir
   - Logs de erro
   - Ambiente (OS, Python version, etc.)

### Contribuindo

Para contribuir com novos testes:

1. Siga a estrutura existente
2. Adicione testes para nova funcionalidade
3. Mantenha cobertura > 80%
4. Execute todos os testes antes do PR

## ðŸ“š Recursos Adicionais

- [Pytest Documentation](https://docs.pytest.org/)
- [Testing Python Applications](https://testandcode.com/)
- [Python Testing with pytest](https://pragprog.com/titles/bopytest/python-testing-with-pytest/)
- [BATS Testing Framework](https://bats-core.readthedocs.io/)

---

## ðŸŽ‰ ConclusÃ£o

Esta suÃ­te de testes garante que o **Cluster AI** mantenha altos padrÃµes de qualidade e confiabilidade. Execute os testes regularmente e contribua com novos testes para manter a saÃºde do projeto!
